import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import json
import time
import logging
import effnetv2_model
from vit_keras import vit

class BaseDNNSplitter:
    """DNNæ¨¡å‹åˆ†å‰²çš„åŸºç¡€ç±»ï¼Œæä¾›å…±äº«åŠŸèƒ½"""
    
    def __init__(self, config):
        """åˆå§‹åŒ–åŸºç¡€åˆ†å‰²å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹ã€ç¯å¢ƒå’Œç®—æ³•å‚æ•°
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.info(f"å¼€å§‹åˆå§‹åŒ–åˆ†å‰²å™¨åŸºç±»ï¼Œæ¨¡å‹: {config['model_name']}, å›¾åƒå¤§å°: {config['image_size']}")
        
        self.config = config
        
        # ç¯å¢ƒå‚æ•°
        self.edge_nodes = config['edge_nodes']
        self.num_edge_nodes = len(self.edge_nodes)
        
        # æ‰¹å¤„ç†å¤§å°é…ç½®å‚æ•°
        if isinstance(config.get('batch_size'), list):
            self.batch_sizes = config['batch_size']  # å­˜å‚¨æ‰¹å¤„ç†å¤§å°åˆ—è¡¨
            self.batch_size = self.batch_sizes[0]    # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå€¼
            self.logger.info(f"æ‰¹å¤„ç†å¤§å°èŒƒå›´: {min(self.batch_sizes)}-{max(self.batch_sizes)}")
        else:
            self.batch_size = config.get('batch_size', 1)
            self.batch_sizes = [self.batch_size]
            self.logger.info(f"æ‰¹å¤„ç†å¤§å°è®¾ç½®ä¸º: {self.batch_size}")
        
        # å¸¦å®½é…ç½®
        self.min_bandwidth = config.get('min_bandwidth', 1 * 10**6)
        self.max_bandwidth = config.get('max_bandwidth', 128 * 10**6)
        self.bandwidth_step = config.get('bandwidth_step', 1 * 10**6)
        
        # ç”Ÿæˆå®Œæ•´çš„å¸¦å®½åˆ—è¡¨(ä»¥Bpsä¸ºå•ä½)
        self.bandwidths = list(range(
            self.min_bandwidth,
            self.max_bandwidth + self.bandwidth_step,
            self.bandwidth_step
        ))
        # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå¸¦å®½å€¼
        self.network_bandwidth = self.bandwidths[0]
        self.logger.info(f"åˆå§‹ç½‘ç»œå¸¦å®½è®¾ç½®ä¸º: {self.network_bandwidth/1e6:.2f} MBps")
        self.logger.info(f"å¸¦å®½èŒƒå›´: {self.min_bandwidth/1e6:.1f}-{self.max_bandwidth/1e6:.1f} MBps, æ­¥é•¿: {self.bandwidth_step/1e6:.1f} MBps")
        self.logger.info(f"æ€»å…± {len(self.bandwidths)} ç§å¸¦å®½è®¾ç½®")
        
        # æ¨¡å‹å‚æ•°
        self.model_name = config['model_name']
        self.image_size = config['image_size']
        self.logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {self.model_name}")
        self.dnn_model = self._load_model()
        
        # è®¡ç®—è¾“å…¥æ•°æ®å¤§å° D
        self.input_data_size = (self.image_size * self.image_size * 3 * 4)  # é«˜*å®½*é€šé“*æ¯åƒç´ å­—èŠ‚æ•°
        self.logger.info(f"è¾“å…¥æ•°æ®å¤§å°: {self.input_data_size/1024:.2f} KB")
        
        # è·å–è‡ªç„¶ç“¶é¢ˆ
        self.logger.info(f"å¼€å§‹è¯†åˆ«æ¨¡å‹è‡ªç„¶ç“¶é¢ˆç‚¹...")
        self.natural_bottlenecks = self._get_natural_bottlenecks(self.dnn_model)
        
        
        # ç¼“å­˜
        self.inference_time_cache = {}
        self.split_models_cache = {}
        
        self.logger.info(f"åˆ†å‰²å™¨åŸºç±»åˆå§‹åŒ–å®Œæˆ")
    
    def _load_model(self):
        """åŠ è½½DNNæ¨¡å‹"""
        if self.model_name == 'vit-b32':
            model = vit.vit_b32(
                image_size=self.image_size,
                activation='sigmoid',
                pretrained=True,
                include_top=True,
                pretrained_top=True
            )
        elif self.model_name == 'vit-l32':
            model = vit.vit_l32(
                image_size=self.image_size,
                activation='sigmoid',
                pretrained=True,
                include_top=True,
                pretrained_top=True
            )
        elif self.model_name == 'vgg-16':
            model = tf.keras.applications.vgg16.VGG16(
                include_top=True,
                weights='imagenet',
                classes=1000,
                classifier_activation='softmax'
            )
        elif self.model_name == 'vgg-19':
            model = tf.keras.applications.vgg19.VGG19(
                include_top=True,
                weights='imagenet',
                classes=1000,
                classifier_activation='softmax'
            )
        else:
            # é»˜è®¤ä½¿ç”¨efficientnetæ¨¡å‹
            # https://github.com/google/automl/blob/master/efficientnetv2/effnetv2_model.py#L578
            input_shape = (self.image_size, self.image_size, 3)
            x = tf.keras.Input(shape=input_shape)
            model = tf.keras.Model(
                inputs=[x], 
                outputs=effnetv2_model.get_model(self.model_name).call(x, training=False)
            )
        
        self.logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹: {self.model_name}")
        return model
    
    def _get_natural_bottlenecks(self, model):
        """è¯†åˆ«æ¨¡å‹ä¸­çš„è‡ªç„¶ç“¶é¢ˆç‚¹
        
        æŒ‰ç…§å…¬å¼: cl = |hl|/|x| 
        å…¶ä¸­|hl|æ˜¯å±‚lçš„ä¸­é—´è¡¨ç¤ºå¤§å°ï¼Œ|x|æ˜¯è¾“å…¥å¤§å°
        å¦‚æœcl < 1ï¼Œåˆ™å±‚læ˜¯DNNçš„è‡ªç„¶ç“¶é¢ˆ
        
        åªä¿ç•™å‹ç¼©æ¯”ä½äºæ‰€æœ‰å…ˆå‰è‡ªç„¶ç“¶é¢ˆçš„è‡ªç„¶ç“¶é¢ˆ
        """
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç“¶é¢ˆç‚¹æ•°æ®æ–‡ä»¶
        output_path = os.path.join('data', f'bottlenecks_{self.model_name}.json')
        if os.path.exists(output_path):
            self.logger.info(f"ä»æ–‡ä»¶åŠ è½½ç“¶é¢ˆç‚¹æ•°æ®: {output_path}")
            with open(output_path, 'r') as f:
                bottlenecks_data = json.load(f)
                
            natural_bottlenecks = []
            for b in bottlenecks_data:
                natural_bottlenecks.append({
                    'layer_name': b['layer_name'],
                    'layer_index': b['layer_index'],
                    'output_size': b['output_size_kb'] * 1024,  # è½¬å›å­—èŠ‚
                    'compression': b['compression']
                })
            
            self.logger.info(f"å·²åŠ è½½ {len(natural_bottlenecks)} ä¸ªè‡ªç„¶ç“¶é¢ˆç‚¹")
            return natural_bottlenecks
        
        # å¦‚æœæ²¡æœ‰å·²å­˜åœ¨çš„æ•°æ®ï¼Œåˆ™é‡æ–°è®¡ç®—
        natural_bottlenecks = []
        input_size = self.input_data_size  # è¾“å…¥æ•°æ®å¤§å° |x|
        best_compression_so_far = 1.0  # è¿½è¸ªåˆ°ç›®å‰ä¸ºæ­¢çš„æœ€ä½³å‹ç¼©æ¯”
        
        self.logger.info(f"å¼€å§‹åˆ†ææ¨¡å‹ç»“æ„ï¼Œå…±æœ‰ {len(model.layers)} å±‚")
        
        # éå†æ‰€æœ‰å±‚è¯†åˆ«è‡ªç„¶ç“¶é¢ˆ
        for i, layer in enumerate(model.layers):
            if not hasattr(layer, 'output_shape') or layer.output_shape is None:
                continue
                
            if isinstance(layer.output_shape, tuple):
                output_shapes = [layer.output_shape]
            else:
                output_shapes = layer.output_shape
                
            for output_shape in output_shapes:
                if len(output_shape) >= 3:  # ç¡®ä¿æ˜¯ç‰¹å¾å›¾å±‚
                    # è®¡ç®—ä¸­é—´è¡¨ç¤ºå¤§å° |hl|
                    if len(output_shape) == 4:  # å·ç§¯å±‚è¾“å‡º
                        output_size = output_shape[1] * output_shape[2] * output_shape[3] * 4  # é«˜*å®½*é€šé“*æ¯å…ƒç´ å­—èŠ‚æ•°
                    else:  # å…¶ä»–ç±»å‹çš„å±‚
                        output_size = np.prod(output_shape[1:]) * 4  # å…¨éƒ¨å…ƒç´ æ•°*æ¯å…ƒç´ å­—èŠ‚æ•°
                    
                    # è®¡ç®—å‹ç¼©æ¯” cl = |hl|/|x|
                    compression = output_size / input_size
                    
                    self.logger.debug(f"å±‚ {i}: {layer.name}, è¾“å‡ºå¤§å°: {output_size/1024:.2f} KB, å‹ç¼©æ¯”: {compression:.4f}")
                    
                    # åªæœ‰å‹ç¼©æ¯”å°äº1ä¸”å°äºä¹‹å‰æ‰€æœ‰ç“¶é¢ˆçš„å‹ç¼©æ¯”ï¼Œæ‰æ˜¯æœ‰ç”¨çš„è‡ªç„¶ç“¶é¢ˆ
                    if compression < 1.0 and compression < best_compression_so_far:
                        best_compression_so_far = compression
                        natural_bottlenecks.append({
                            'layer_name': layer.name,
                            'layer_index': i,
                            'output_size': output_size,
                            'compression': compression
                        })
                        self.logger.info(f"æ‰¾åˆ°è‡ªç„¶ç“¶é¢ˆç‚¹: å±‚ {i} - {layer.name}, å‹ç¼©æ¯”: {compression:.4f}")
        
        # æŒ‰å±‚ç´¢å¼•æ’åº
        natural_bottlenecks = sorted(natural_bottlenecks, key=lambda x: x['layer_index'])
        self.logger.info(f"æ€»å…±è¯†åˆ«åˆ° {len(natural_bottlenecks)} ä¸ªæœ‰æ•ˆè‡ªç„¶ç“¶é¢ˆç‚¹")
        
        bottlenecks_data = [
            {
                'layer_name': b['layer_name'],
                'layer_index': b['layer_index'],
                'output_size_kb': b['output_size']/1024,
                'compression': b['compression']
            } for b in natural_bottlenecks
        ]
        
        with open(output_path, 'w') as f:
            json.dump(bottlenecks_data, f, indent=2)
        
        self.logger.info(f"ç“¶é¢ˆç‚¹æ•°æ®å·²ä¿å­˜åˆ° {output_path}")
        return natural_bottlenecks
    
    def get_split_models(self, bottleneck):
        """è·å–æŒ‡å®šç“¶é¢ˆç‚¹çš„å¤´éƒ¨å’Œå°¾éƒ¨æ¨¡å‹"""
        cache_key = bottleneck['layer_name']
        
        if cache_key in self.split_models_cache:
            return self.split_models_cache[cache_key]
            
        # åˆ›å»ºå¤´éƒ¨æ¨¡å‹
        head_model = tf.keras.models.Model(
            inputs=self.dnn_model.inputs,
            outputs=self.dnn_model.get_layer(bottleneck['layer_name']).output
        )
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºResNet50æ¨¡å‹
        if self.model_name == 'ResNet50':
            return self._get_resnet50_split_models(bottleneck)
        
        # éResNet50æ¨¡å‹çš„å¤„ç†é€»è¾‘ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        next_layer_index = bottleneck['layer_index'] + 1
        tail_input_shape = self.dnn_model.layers[next_layer_index].input_shape
        
        if isinstance(tail_input_shape, list):
            tail_input_shape = tail_input_shape[0]
            
        tail_input = tf.keras.Input(shape=tail_input_shape[1:])
        x = tail_input
        for layer in self.dnn_model.layers[next_layer_index:]:
            # å¤„ç†å…ƒç»„ç±»å‹çš„è¾“å…¥
            if isinstance(x, tuple):
                x = layer(x[0])
            else:
                x = layer(x)
        tail_model = tf.keras.models.Model(inputs=tail_input, outputs=x)
        
        self.split_models_cache[cache_key] = (head_model, tail_model)
        return head_model, tail_model

    def _get_resnet50_split_models(self, bottleneck):
        """ä¸“é—¨ä¸ºResNet50æ¨¡å‹åˆ›å»ºåˆ†å‰²æ¨¡å‹
        
        Args:
            bottleneck: ç“¶é¢ˆç‚¹ä¿¡æ¯
                
        Returns:
            (head_model, tail_model): åˆ†å‰²åçš„å¤´éƒ¨å’Œå°¾éƒ¨æ¨¡å‹
        """
        self.logger.info(f"ä¸ºResNet50åœ¨ç“¶é¢ˆç‚¹ {bottleneck['layer_name']} (å±‚ç´¢å¼•: {bottleneck['layer_index']}) åˆ›å»ºåˆ†å‰²æ¨¡å‹...")
        
        # 1. åˆ›å»ºå¤´éƒ¨æ¨¡å‹ - ä»è¾“å…¥åˆ°ç“¶é¢ˆå±‚
        head_model = tf.keras.models.Model(
            inputs=self.dnn_model.inputs,
            outputs=self.dnn_model.get_layer(bottleneck['layer_name']).output
        )
        self.logger.info(f"å¤´éƒ¨æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {head_model.output_shape}")
        
        # 2. åˆ›å»ºå°¾éƒ¨æ¨¡å‹ - ä»ç“¶é¢ˆå±‚åˆ°è¾“å‡º
        # è·å–ç“¶é¢ˆå±‚è¾“å‡ºå½¢çŠ¶
        output_shape = head_model.output_shape
        
        # åˆ›å»ºæ–°çš„è¾“å…¥å±‚ï¼ŒåŒ¹é…ç“¶é¢ˆå±‚çš„è¾“å‡ºå½¢çŠ¶
        tail_input = tf.keras.Input(shape=output_shape[1:], name=f"{bottleneck['layer_name']}_input")
        
        # è·å–ç“¶é¢ˆå±‚çš„ä¿¡æ¯
        bottleneck_layer = self.dnn_model.get_layer(bottleneck['layer_name'])
        
        # æ ¹æ®ä¸åŒçš„ç“¶é¢ˆç‚¹é€‰æ‹©ä¸åŒçš„å°¾éƒ¨ç»“æ„
        if bottleneck['layer_name'] == "conv3_block1_1_conv":
            # ä¸ºconv3_block1_1_convåˆ›å»ºç‰¹å®šç»“æ„
            # é¦–å…ˆè¦åˆ›å»ºå·ç§¯å—çš„å‰©ä½™éƒ¨åˆ†
            x = tf.keras.layers.BatchNormalization(name='conv3_block1_1_bn')(tail_input)
            x = tf.keras.layers.Activation('relu', name='conv3_block1_1_relu')(x)
            
            # æ·»åŠ æ®‹å·®è¿æ¥çš„åˆå¹¶ç‚¹
            # è¿™é‡Œéœ€è¦ç¡®ä¿å°ºå¯¸åŒ¹é…ï¼Œå¯èƒ½éœ€è¦ä¸€ä¸ªé¢å¤–çš„1x1å·ç§¯è½¬æ¢é€šé“æ•°
            shortcut = tf.keras.layers.Conv2D(512, (1, 1), strides=(2, 2), name='conv3_block1_0_conv')(x)
            shortcut = tf.keras.layers.BatchNormalization(name='conv3_block1_0_bn')(shortcut)
            
            # ç»§ç»­ä¸»è·¯å¾„
            x = tf.keras.layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same', name='conv3_block1_2_conv')(x)
            x = tf.keras.layers.BatchNormalization(name='conv3_block1_2_bn')(x)
            x = tf.keras.layers.Activation('relu', name='conv3_block1_2_relu')(x)
            x = tf.keras.layers.Conv2D(512, (1, 1), name='conv3_block1_3_conv')(x)
            x = tf.keras.layers.BatchNormalization(name='conv3_block1_3_bn')(x)
            
            # æ·»åŠ æ®‹å·®è¿æ¥
            x = tf.keras.layers.Add(name='conv3_block1_add')([shortcut, x])
            x = tf.keras.layers.Activation('relu', name='conv3_block1_out')(x)
            
            # æ·»åŠ åç»­å—çš„æ¨¡æ‹Ÿå±‚
            # è¿™é‡Œä¸å†è¯¦ç»†å®ç°æ¯ä¸ªå—ï¼Œä½¿ç”¨å‡ ä¸ªå·ç§¯å±‚æ¨¡æ‹Ÿåç»­å¤„ç†
            x = tf.keras.layers.Conv2D(512, (1, 1), name='conv3_sim')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Activation('relu')(x)
            
            # conv4æ¨¡æ‹Ÿ
            x = tf.keras.layers.Conv2D(1024, (1, 1), strides=(2, 2), name='conv4_sim')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Activation('relu')(x)
            
            # conv5æ¨¡æ‹Ÿ
            x = tf.keras.layers.Conv2D(2048, (1, 1), strides=(2, 2), name='conv5_sim')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Activation('relu')(x)
            
        elif bottleneck['layer_name'] == "conv4_block1_1_conv":
            # ä¸ºconv4_block1_1_convåˆ›å»ºç‰¹å®šç»“æ„
            x = tf.keras.layers.BatchNormalization(name='conv4_block1_1_bn')(tail_input)
            x = tf.keras.layers.Activation('relu', name='conv4_block1_1_relu')(x)
            
            # æ·»åŠ æ®‹å·®è¿æ¥çš„åˆå¹¶ç‚¹
            shortcut = tf.keras.layers.Conv2D(1024, (1, 1), strides=(2, 2), name='conv4_block1_0_conv')(x)
            shortcut = tf.keras.layers.BatchNormalization(name='conv4_block1_0_bn')(shortcut)
            
            # ç»§ç»­ä¸»è·¯å¾„
            x = tf.keras.layers.Conv2D(1024, (3, 3), strides=(2, 2), padding='same', name='conv4_block1_2_conv')(x)
            x = tf.keras.layers.BatchNormalization(name='conv4_block1_2_bn')(x)
            x = tf.keras.layers.Activation('relu', name='conv4_block1_2_relu')(x)
            x = tf.keras.layers.Conv2D(1024, (1, 1), name='conv4_block1_3_conv')(x)
            x = tf.keras.layers.BatchNormalization(name='conv4_block1_3_bn')(x)
            
            # æ·»åŠ æ®‹å·®è¿æ¥
            x = tf.keras.layers.Add(name='conv4_block1_add')([shortcut, x])
            x = tf.keras.layers.Activation('relu', name='conv4_block1_out')(x)
            
            # æ·»åŠ conv5æ¨¡æ‹Ÿ
            x = tf.keras.layers.Conv2D(2048, (1, 1), strides=(2, 2), name='conv5_sim')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Activation('relu')(x)
            
        elif bottleneck['layer_name'] == "conv5_block1_1_conv":
            # ä¸ºconv5_block1_1_convåˆ›å»ºç‰¹å®šç»“æ„
            x = tf.keras.layers.BatchNormalization(name='conv5_block1_1_bn')(tail_input)
            x = tf.keras.layers.Activation('relu', name='conv5_block1_1_relu')(x)
            
            # æ·»åŠ æ®‹å·®è¿æ¥çš„åˆå¹¶ç‚¹
            shortcut = tf.keras.layers.Conv2D(2048, (1, 1), strides=(2, 2), name='conv5_block1_0_conv')(x)
            shortcut = tf.keras.layers.BatchNormalization(name='conv5_block1_0_bn')(shortcut)
            
            # ç»§ç»­ä¸»è·¯å¾„
            x = tf.keras.layers.Conv2D(2048, (3, 3), strides=(2, 2), padding='same', name='conv5_block1_2_conv')(x)
            x = tf.keras.layers.BatchNormalization(name='conv5_block1_2_bn')(x)
            x = tf.keras.layers.Activation('relu', name='conv5_block1_2_relu')(x)
            x = tf.keras.layers.Conv2D(2048, (1, 1), name='conv5_block1_3_conv')(x)
            x = tf.keras.layers.BatchNormalization(name='conv5_block1_3_bn')(x)
            
            # æ·»åŠ æ®‹å·®è¿æ¥
            x = tf.keras.layers.Add(name='conv5_block1_add')([shortcut, x])
            x = tf.keras.layers.Activation('relu', name='conv5_block1_out')(x)
        
        else:
            # å¦‚æœä¸æ˜¯é¢„å®šä¹‰çš„åˆ†å‰²ç‚¹ï¼Œç»™å‡ºè­¦å‘Šå¹¶ä½¿ç”¨ç®€åŒ–å®ç°
            self.logger.warning(f"åˆ†å‰²ç‚¹ {bottleneck['layer_name']} ä¸æ˜¯é¢„å®šä¹‰çš„ResNet50åˆ†å‰²ç‚¹ä¹‹ä¸€ã€‚ä½¿ç”¨ç®€åŒ–æ¨¡å‹ã€‚")
            # ç®€å•çš„å·ç§¯+æ‰¹å½’ä¸€åŒ–+æ¿€æ´»å•å…ƒ
            x = tf.keras.layers.Conv2D(512, (1, 1))(tail_input)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Activation('relu')(x)
        
        # å…±äº«çš„æœ€ç»ˆå±‚å¤„ç†
        x = tf.keras.layers.GlobalAveragePooling2D()(x)
        x = tf.keras.layers.Dense(1000, activation='softmax')(x)
        
        # åˆ›å»ºæœ€ç»ˆçš„å°¾éƒ¨æ¨¡å‹
        tail_model = tf.keras.models.Model(inputs=tail_input, outputs=x)
        self.logger.info(f"å°¾éƒ¨æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œè¾“å…¥å½¢çŠ¶: {tail_input.shape}, è¾“å‡ºå½¢çŠ¶: {x.shape}")
        
        # ç¼“å­˜æ¨¡å‹ä»¥é¿å…é‡å¤åˆ›å»º
        self.split_models_cache[bottleneck['layer_name']] = (head_model, tail_model)
        return head_model, tail_model
    
    def measure_cloud_only(self):
        """æµ‹é‡æ•´ä¸ªæ¨¡å‹åœ¨äº‘ç«¯æ‰§è¡Œçš„æ¨ç†æ—¶å»¶å’Œæˆæœ¬
        
        è€ƒè™‘å°†è¾“å…¥æ•°æ®ä»è¾¹ç¼˜ä¼ è¾“åˆ°äº‘ç«¯çš„æ—¶é—´ï¼Œä»¥åŠäº‘ç«¯æ‰§è¡Œçš„æ—¶é—´
        
        Returns:
            åŒ…å«æ¨ç†æ—¶é—´å’Œæˆæœ¬ä¿¡æ¯çš„å­—å…¸
        """
        cache_key = f"cloud_only_{self.batch_size}"
        
        if cache_key in self.inference_time_cache:
            self.logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„äº‘ç«¯æ¨ç†æ—¶é—´: {cache_key}")
            return self.inference_time_cache[cache_key]
        
        self.logger.info(f"æµ‹é‡åœ¨äº‘ç«¯å…¨é‡æ‰§è¡Œçš„æ¨ç†æ—¶é—´")
        
        # ä¸ºæµ‹é‡å‡†å¤‡è¾“å…¥æ•°æ®
        input_shape = (self.batch_size, self.image_size, self.image_size, 3)
        input_data = tf.ones(input_shape)
        
        # è®¡ç®—ä¼ è¾“è¾“å…¥æ•°æ®çš„æ—¶é—´
        input_data_size = self.batch_size * self.input_data_size
        trans_time = input_data_size / self.network_bandwidth
        self.logger.info(f"è¾“å…¥æ•°æ®å¤§å°: {input_data_size/1024:.2f} KB, ä¼ è¾“æ—¶é—´: {trans_time*1000:.2f} ms")
        
        # æµ‹é‡äº‘ç«¯æ‰§è¡Œæ—¶é—´
        with tf.device(self.config['cloud_device']):
            # é¢„çƒ­
            for _ in range(5):
                _ = self.dnn_model(input_data)
            
            # æµ‹é‡æ‰§è¡Œæ—¶é—´
            start_time = time.time()
            for _ in range(10):
                _ = self.dnn_model(input_data)
            cloud_time = (time.time() - start_time) / 10
        
        self.logger.info(f"äº‘ç«¯æ‰§è¡Œæ—¶é—´: {cloud_time*1000:.2f} ms")
        
        # è®¡ç®—æ€»æ—¶å»¶å’Œæˆæœ¬
        total_time = trans_time + cloud_time
        
        self.logger.info(f"æ€»æ—¶å»¶: {total_time*1000:.2f} ms")
        
        result = {
            'edge_time': 0,
            'cloud_time': cloud_time,
            'trans_time': trans_time,
            'total_time': total_time,
            'split_type': 'cloud_only'
        }
        
        self.inference_time_cache[cache_key] = result
        return result
    
    def measure_inference_time(self, edge_node, bottleneck=None):
        """æµ‹é‡æŒ‡å®šåˆ†å‰²ç‚¹çš„æ¨ç†æ—¶å»¶å’Œæˆæœ¬
        
        åŸºäºå…¬å¼: ğ‘‡_ğ‘™ = ğ‘‡_(1,ğ‘™)^â„ + (ğ·âˆ—ğ‘_l)/ğ‘Ÿ + ğ‘‡_(ğ‘™+1,ğ¿)^ğ‘¡
        
        Args:
            edge_node: è¾¹ç¼˜èŠ‚ç‚¹é…ç½®
            bottleneck: åˆ†å‰²ç‚¹ä¿¡æ¯ï¼ŒNoneè¡¨ç¤ºä¸åˆ†å‰²
            
        Returns:
            åŒ…å«æ¨ç†æ—¶é—´å’Œæˆæœ¬ä¿¡æ¯çš„å­—å…¸
        """
        device_key = edge_node['device'].replace('/', '_')
        cache_key = f"{device_key}_{self.batch_size}"
        if bottleneck:
            cache_key += f"_{bottleneck['layer_name']}"
            
        if cache_key in self.inference_time_cache:
            self.logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æ¨ç†æ—¶é—´: {cache_key}")
            return self.inference_time_cache[cache_key]
        
        # ä¸ºæµ‹é‡å‡†å¤‡è¾“å…¥æ•°æ®
        input_shape = (self.batch_size, self.image_size, self.image_size, 3)
        input_data = tf.ones(input_shape)
        
        # 1. å…¨éƒ¨åœ¨è¾¹ç¼˜èŠ‚ç‚¹æ‰§è¡Œ
        if bottleneck is None:
            self.logger.info(f"æµ‹é‡åœ¨è¾¹ç¼˜èŠ‚ç‚¹ {edge_node['name']} å…¨é‡æ‰§è¡Œçš„æ¨ç†æ—¶é—´")
            self.logger.info(f"æŒ‡å®šè®¾å¤‡: {edge_node['device']}")
            
            # è®°å½•TensorFlowå¯è§çš„è®¾å¤‡
            self.logger.info(f"TensorFlowå¯è§è®¾å¤‡: {tf.config.list_physical_devices()}")
            
            with tf.device(edge_node['device']):
                # é¢„çƒ­
                for _ in range(5):
                    _ = self.dnn_model(input_data)
                
                # æµ‹é‡æ‰§è¡Œæ—¶é—´
                start_time = time.time()
                for _ in range(10):
                    _ = self.dnn_model(input_data)
                edge_time = (time.time() - start_time) / 10
                
            self.logger.info(f"è¾¹ç¼˜æ‰§è¡Œæ—¶é—´: {edge_time*1000:.2f} ms")
            
            result = {
                'edge_time': edge_time,  # ğ‘‡_(1,ğ¿)^â„
                'cloud_time': 0,
                'trans_time': 0,
                'total_time': edge_time,
                'split_type': 'no_split'
            }
            
        # 2. åœ¨æŒ‡å®šç‚¹åˆ†å‰²æ‰§è¡Œ
        else:
            self.logger.info(f"æµ‹é‡åœ¨è¾¹ç¼˜èŠ‚ç‚¹ {edge_node['name']} æ‰§è¡Œåˆ° {bottleneck['layer_name']} å±‚ååˆ†å‰²çš„æ¨ç†æ—¶é—´")
            head_model, tail_model = self.get_split_models(bottleneck)
            
            # æµ‹é‡è¾¹ç¼˜ç«¯æ‰§è¡Œå¤´éƒ¨æ¨¡å‹çš„æ—¶é—´ ğ‘‡_(1,ğ‘™)^â„
            with tf.device(edge_node['device']):
                # é¢„çƒ­
                for _ in range(5):
                    head_output = head_model(input_data)
                
                # æµ‹é‡æ‰§è¡Œæ—¶é—´
                start_time = time.time()
                for _ in range(10):
                    head_output = head_model(input_data)
                edge_time = (time.time() - start_time) / 10
            
            self.logger.info(f"è¾¹ç¼˜å¤´éƒ¨æ‰§è¡Œæ—¶é—´: {edge_time*1000:.2f} ms")
            
            # è®¡ç®—ä¼ è¾“ä¸­é—´ç»“æœçš„æ—¶é—´ (ğ·âˆ—ğ‘_l)/ğ‘Ÿ
            if isinstance(head_output, tuple):
                data_size = head_output[0].shape.num_elements() * 4  # float32 = 4 bytes
            else:
                data_size = head_output.shape.num_elements() * 4
            trans_time = data_size / self.network_bandwidth
            
            self.logger.info(f"ä¸­é—´ç»“æœå¤§å°: {data_size/1024:.2f} KB, ä¼ è¾“æ—¶é—´: {trans_time*1000:.2f} ms")
            
            # æµ‹é‡äº‘ç«¯æ‰§è¡Œå°¾éƒ¨æ¨¡å‹çš„æ—¶é—´ ğ‘‡_(ğ‘™+1,ğ¿)^ğ‘¡
            with tf.device(self.config['cloud_device']):
                if isinstance(head_output, tuple):
                    tail_input = tf.ones_like(head_output[0])
                else:
                    tail_input = tf.ones_like(head_output)
                    
                # é¢„çƒ­
                for _ in range(5):
                    _ = tail_model(tail_input)
                
                # æµ‹é‡æ‰§è¡Œæ—¶é—´
                start_time = time.time()
                for _ in range(10):
                    _ = tail_model(tail_input)
                cloud_time = (time.time() - start_time) / 10
            
            self.logger.info(f"äº‘ç«¯å°¾éƒ¨æ‰§è¡Œæ—¶é—´: {cloud_time*1000:.2f} ms")
            
            # è®¡ç®—æ€»æ—¶å»¶å’Œæˆæœ¬
            total_time = edge_time + cloud_time + trans_time  # ğ‘‡_ğ‘™
            
            self.logger.info(f"æ€»æ—¶å»¶: {total_time*1000:.2f} ms")
            
            result = {
                'edge_time': edge_time,
                'cloud_time': cloud_time,
                'trans_time': trans_time,
                'total_time': total_time,
                'split_type': 'split',
                'bottleneck': bottleneck['layer_name'],
                'compression': bottleneck['compression'],
                'data_size': data_size
            }
            
        self.inference_time_cache[cache_key] = result
        return result
    
    def calculate_reward(self, result):
        """è®¡ç®—å¥–åŠ±å€¼
        
        ä½¿ç”¨å…¬å¼: R = -((1 - omega) * T_all + omega * C_all)
        """
        latency = result.get('total_time', 0)
    
        # åŸºç¡€å¥–åŠ±ï¼šå»¶è¿Ÿè¶Šä½ï¼Œå¥–åŠ±è¶Šé«˜
        base_reward = -latency * 25  # æ”¾å¤§10å€

        return base_reward
    
    def find_optimal_split(self):
        """æ ¹æ®å½“å‰ç½‘ç»œå¸¦å®½å’Œè®¡ç®—èµ„æºæ‰¾åˆ°æœ€ä¼˜åˆ†å‰²ç­–ç•¥
        
        ä½¿ç”¨å…¬å¼: Sopt = argmin_{lâˆˆ{0...L}} (T_l)
        """
        self.logger.info(f"å½“å‰ç½‘ç»œå¸¦å®½: {self.network_bandwidth/1e6:.2f} MBps")
        self.logger.info(f"å¯»æ‰¾æœ€ä¼˜åˆ†å‰²ç‚¹...")
        
        print(f"å½“å‰ç½‘ç»œå¸¦å®½: {self.network_bandwidth/1e6:.2f} MBps")
        print(f"å¯»æ‰¾æœ€ä¼˜åˆ†å‰²ç‚¹...")
        
        options = []
        rewards = []
        
        # å…ˆè¯„ä¼°å…¨éƒ¨åœ¨äº‘ç«¯æ‰§è¡Œçš„æ€§èƒ½
        cloud_result = self.measure_cloud_only()
        cloud_reward = self.calculate_reward(cloud_result)
        options.append({
            'strategy': 'cloud_only',
            'reward': cloud_reward,
            'result': cloud_result
        })
        rewards.append(cloud_reward)
        
        # è¯„ä¼°æ¯ä¸ªè¾¹ç¼˜èŠ‚ç‚¹å®Œå…¨æ‰§è¡Œçš„æ€§èƒ½
        for i, edge_node in enumerate(self.edge_nodes):
            edge_result = self.measure_inference_time(edge_node)
            edge_reward = self.calculate_reward(edge_result)
            options.append({
                'strategy': f"edge_only_{i}",
                'edge_node': edge_node['name'],
                'reward': edge_reward,
                'result': edge_result
            })
            rewards.append(edge_reward)
        
        # è¯„ä¼°æ¯ä¸ªåˆ†å‰²ç‚¹åœ¨æ¯ä¸ªè¾¹ç¼˜èŠ‚ç‚¹ä¸Šçš„æ€§èƒ½
        for i, edge_node in enumerate(self.edge_nodes):
            for bottleneck in self.natural_bottlenecks:
                split_result = self.measure_inference_time(edge_node, bottleneck)
                split_reward = self.calculate_reward(split_result)
                options.append({
                    'strategy': 'split',
                    'edge_node': edge_node['name'],
                    'bottleneck': bottleneck['layer_name'],
                    'compression': bottleneck['compression'],
                    'reward': split_reward,
                    'result': split_result
                })
                rewards.append(split_reward)
        
        # æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥
        best_index = np.argmax(rewards)
        best_option = options[best_index]
        
        print("\næœ€ä¼˜åˆ†å‰²ç­–ç•¥:")
        self.logger.info(f"æœ€ä¼˜åˆ†å‰²ç­–ç•¥:")
        if best_option['strategy'] == 'cloud_only':
            self.logger.info(f"ç­–ç•¥: å…¨éƒ¨åœ¨äº‘ç«¯æ‰§è¡Œ:")
            print("ç­–ç•¥: å…¨éƒ¨åœ¨äº‘ç«¯æ‰§è¡Œ")
        elif best_option['strategy'].startswith('edge_only'):
            self.logger.info(f"ç­–ç•¥: å…¨éƒ¨åœ¨è¾¹ç¼˜èŠ‚ç‚¹ {best_option['edge_node']} æ‰§è¡Œ")
            print(f"ç­–ç•¥: å…¨éƒ¨åœ¨è¾¹ç¼˜èŠ‚ç‚¹ {best_option['edge_node']} æ‰§è¡Œ")
        else:
            self.logger.info(f"ç­–ç•¥: åœ¨è¾¹ç¼˜èŠ‚ç‚¹ {best_option['edge_node']} æ‰§è¡Œåˆ° {best_option['bottleneck']} å±‚ï¼Œç„¶åä¼ è¾“åˆ°äº‘ç«¯")
            self.logger.info(f"å‹ç¼©ç‡: {best_option['compression']:.4f}")
            print(f"ç­–ç•¥: åœ¨è¾¹ç¼˜èŠ‚ç‚¹ {best_option['edge_node']} æ‰§è¡Œåˆ° {best_option['bottleneck']} å±‚ï¼Œç„¶åä¼ è¾“åˆ°äº‘ç«¯")
            print(f"å‹ç¼©ç‡: {best_option['compression']:.4f}")
        
        self.logger.info(f"æ€»æ—¶å»¶: {best_option['result']['total_time']*1000:.2f} ms")
        print(f"æ€»æ—¶å»¶: {best_option['result']['total_time']*1000:.2f} ms")
        
        return best_option
    
    def build_dqn_model(self, state_size=None):
        """æ„å»ºæ ‡å‡†DQNæ¨¡å‹"""
        if state_size is None:
            state_size = 2 + len(self.natural_bottlenecks)
        
        action_size = len(self.natural_bottlenecks) + 2  # è‡ªç„¶ç“¶é¢ˆç‚¹ + å…¨äº‘ç«¯ + å…¨è¾¹ç¼˜
        
        # æ„å»ºç½‘ç»œ
        inputs = tf.keras.layers.Input(shape=(state_size,))
        x = tf.keras.layers.Dense(128, activation='relu')(inputs)
        x = tf.keras.layers.Dense(128, activation='relu')(x)
        
        # ä½¿ç”¨é»˜è®¤åˆå§‹åŒ–çš„è¾“å‡ºå±‚
        outputs = tf.keras.layers.Dense(action_size, activation='linear')(x)
        
        model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
        
        # ä½¿ç”¨HuberæŸå¤±å’Œé€‚å½“å­¦ä¹ ç‡
        model.compile(
            loss=tf.keras.losses.Huber(delta=100.0),
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005)
        )
        
        self.logger.info(f"æ„å»ºæ ‡å‡†DQNæ¨¡å‹: çŠ¶æ€ç»´åº¦={state_size}, åŠ¨ä½œç»´åº¦={action_size}")
        return model
        
    def get_state(self):
        """è·å–å½“å‰ç¯å¢ƒçŠ¶æ€ï¼ŒåŒ…å«æ‰¹å¤„ç†å¤§å°å’Œå¸¦å®½ä¿¡æ¯
        
        Returns:
            è¡¨ç¤ºå½“å‰ç¯å¢ƒçŠ¶æ€çš„numpyæ•°ç»„
        """
        # å½’ä¸€åŒ–å¸¦å®½ (1-128MBpsèŒƒå›´å†…)
        norm_bandwidth = self.network_bandwidth / (128 * 10**6)
        
        # å½’ä¸€åŒ–æ‰¹å¤„ç†å¤§å° (1-64èŒƒå›´å†…)
        max_batch_size = 64.0
        norm_batch_size = min(self.batch_size / max_batch_size, 1.0)
        
        # è‡ªç„¶ç“¶é¢ˆç‚¹çš„å‹ç¼©ç‡
        bottleneck_compressions = [bottleneck['compression'] for bottleneck in self.natural_bottlenecks]
        
        # ç»„åˆçŠ¶æ€
        state = [norm_bandwidth, norm_batch_size] + bottleneck_compressions
        
        return np.reshape(np.array(state), [1, 2 + len(self.natural_bottlenecks)])
    
    def save_model_weights(self, model, file_path):
        """ä¿å­˜æ¨¡å‹æƒé‡åˆ°æ–‡ä»¶
        
        Args:
            model: è¦ä¿å­˜çš„æ¨¡å‹
            file_path: ä¿å­˜è·¯å¾„
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹æƒé‡
            model.save_weights(file_path)
            self.logger.info(f"æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ°: {file_path}")
            return True
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ¨¡å‹æƒé‡å¤±è´¥: {str(e)}")
            return False
    
    def load_model_weights(self, model, file_path):
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡
        
        Args:
            model: è¦åŠ è½½æƒé‡çš„æ¨¡å‹
            file_path: æƒé‡æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        if os.path.exists(file_path):
            try:
                model.load_weights(file_path)
                self.logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {file_path}")
                return True
            except Exception as e:
                self.logger.error(f"åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {str(e)}")
                return False
        else:
            self.logger.warning(f"æ¨¡å‹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
    
    def predict_action_with_qvalues(self, state, model):
        """ä½¿ç”¨ç»™å®šæ¨¡å‹é¢„æµ‹åŠ¨ä½œå’Œå¯¹åº”çš„Qå€¼
        
        Args:
            state: å½“å‰çŠ¶æ€
            model: DQNæ¨¡å‹
            
        Returns:
            action: é¢„æµ‹çš„åŠ¨ä½œ
            q_values: æ‰€æœ‰åŠ¨ä½œçš„Qå€¼
        """
        q_values = model.predict(state)[0]
        action = np.argmax(q_values)
        return action, q_values
    
    def interpret_action(self, action):
        """è§£é‡ŠåŠ¨ä½œçš„å«ä¹‰
        
        Args:
            action: åŠ¨ä½œç´¢å¼•
            
        Returns:
            åŒ…å«åŠ¨ä½œè§£é‡Šçš„å­—å…¸
        """
        if action == 0:
            return {
                'strategy': 'cloud_only',
                'description': 'å…¨éƒ¨åœ¨äº‘ç«¯æ‰§è¡Œ'
            }
        elif action == 1:
            return {
                'strategy': 'edge_only',
                'edge_node': self.edge_nodes[0]['name'],
                'description': f"å…¨éƒ¨åœ¨è¾¹ç¼˜èŠ‚ç‚¹ {self.edge_nodes[0]['name']} æ‰§è¡Œ"
            }
        else:
            bottleneck_idx = action - 2
            if bottleneck_idx < len(self.natural_bottlenecks):
                bottleneck = self.natural_bottlenecks[bottleneck_idx]
                return {
                    'strategy': 'split',
                    'edge_node': self.edge_nodes[0]['name'],
                    'bottleneck': bottleneck['layer_name'],
                    'compression': bottleneck['compression'],
                    'description': f"åœ¨è¾¹ç¼˜èŠ‚ç‚¹ {self.edge_nodes[0]['name']} æ‰§è¡Œåˆ° {bottleneck['layer_name']} å±‚ï¼Œç„¶åä¼ è¾“åˆ°äº‘ç«¯"
                }
            else:
                return {
                    'strategy': 'error',
                    'description': 'æ— æ•ˆçš„åŠ¨ä½œç´¢å¼•'
                }
    
    def execute_action(self, action):
        """æ‰§è¡ŒæŒ‡å®šçš„åŠ¨ä½œï¼Œè·å–å®é™…æ€§èƒ½æ•°æ®
        
        Args:
            action: åŠ¨ä½œç´¢å¼•
            
        Returns:
            åŠ¨ä½œæ‰§è¡Œç»“æœ
        """
        action_info = self.interpret_action(action)
        
        if action_info['strategy'] == 'cloud_only':
            result = self.measure_cloud_only()
        elif action_info['strategy'] == 'edge_only':
            result = self.measure_inference_time(self.edge_nodes[0])
        elif action_info['strategy'] == 'split':
            # æ‰¾åˆ°å¯¹åº”çš„ç“¶é¢ˆç‚¹
            bottleneck = next(b for b in self.natural_bottlenecks 
                             if b['layer_name'] == action_info['bottleneck'])
            result = self.measure_inference_time(self.edge_nodes[0], bottleneck)
        else:
            # é”™è¯¯æƒ…å†µï¼Œé»˜è®¤ä½¿ç”¨è¾¹ç¼˜æ‰§è¡Œ
            self.logger.error(f"æ— æ•ˆåŠ¨ä½œ: {action}ï¼Œä½¿ç”¨é»˜è®¤è¾¹ç¼˜æ‰§è¡Œ")
            result = self.measure_inference_time(self.edge_nodes[0])
            
        return {**action_info, 'result': result}
    def describe_action(self, action):
        """å°†åŠ¨ä½œç´¢å¼•è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æè¿°
        
        Args:
            action: åŠ¨ä½œç´¢å¼•ï¼ˆæ•´æ•°ï¼‰
            
        Returns:
            str: åŠ¨ä½œçš„æ–‡æœ¬æè¿°
        """
        if action is None:
            return "æœªçŸ¥åŠ¨ä½œ"
        
        # ç‰¹æ®ŠåŠ¨ä½œï¼šå…¨éƒ¨åœ¨è¾¹ç¼˜æˆ–å…¨éƒ¨åœ¨äº‘ç«¯
        if action == 0:
            return "å…¨éƒ¨åœ¨äº‘ç«¯æ‰§è¡Œï¼ˆä¸åˆ†å‰²ï¼‰"
        elif action == 1:
            return "å…¨éƒ¨åœ¨è¾¹ç¼˜è®¾å¤‡æ‰§è¡Œï¼ˆä¸åˆ†å‰²ï¼‰"
        
        # æ­£å¸¸åˆ†å‰²åŠ¨ä½œ
        bottleneck_idx = action - 2  # å‡å»ç‰¹æ®ŠåŠ¨ä½œçš„æ•°é‡
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
        if bottleneck_idx < 0 or bottleneck_idx >= len(self.natural_bottlenecks):
            return f"æ— æ•ˆåŠ¨ä½œ (ç´¢å¼•={action})"
        
        # è¿”å›å…·ä½“çš„åˆ†å‰²ç‚¹æè¿°
        bottleneck = self.natural_bottlenecks[bottleneck_idx]
        return f"åœ¨ {bottleneck} å¤„åˆ†å‰²"
    def ensure_directories(self):
        """ç¡®ä¿æ•°æ®å’Œç»“æœç›®å½•å­˜åœ¨"""
        os.makedirs('data', exist_ok=True)
        os.makedirs('models', exist_ok=True)
        os.makedirs('result', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        self.logger.info("åˆ›å»ºæ•°æ®ã€æ¨¡å‹ã€ç»“æœå’Œæ—¥å¿—ç›®å½•")
    
    def visualize_split_performance(self, results, save_path=None):
        """å¯è§†åŒ–ä¸åŒåˆ†å‰²ç‚¹çš„æ€§èƒ½å¯¹æ¯”
        
        Args:
            results: åŒ…å«ä¸åŒåˆ†å‰²ç‚¹æ€§èƒ½æ•°æ®çš„åˆ—è¡¨
            save_path: å›¾è¡¨ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸ºNoneè‡ªåŠ¨ç”Ÿæˆ
        """
        if not results:
            self.logger.warning("æ²¡æœ‰æä¾›æ€§èƒ½æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå¯è§†åŒ–")
            return None
        
        # æå–æ•°æ®
        split_names = []
        latencies = []
        
        for result in results:
            if result['strategy'] == 'cloud_only':
                name = 'Cloud Only'
            elif result['strategy'].startswith('edge_only'):
                name = f"Edge ({result['edge_node']})"
            else:
                name = f"Split ({result.get('bottleneck', 'unknown')})"
                
            split_names.append(name)
            latencies.append(result['result']['total_time'] * 1000)  # è½¬æ¢ä¸ºms
        
        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ç»˜åˆ¶æ—¶å»¶å›¾
        bars1 = ax1.bar(split_names, latencies, color='skyblue')
        ax1.set_title('Inference Latency Comparison', fontsize=14)
        ax1.set_ylabel('Latency (ms)', fontsize=12)
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(axis='y', linestyle='--', alpha=0.7)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars1:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{height:.2f}', ha='center', fontsize=10)
        
        
        plt.suptitle(f'DNN Split Performance Analysis - {self.model_name}', fontsize=16)
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        if save_path is None:
            save_path = os.path.join('result', f'split_performance_{self.model_name}_{self.batch_size}_{int(self.network_bandwidth/1e6)}mbps.png')
        
        plt.savefig(save_path, dpi=150)
        plt.close()
        
        self.logger.info(f"åˆ†å‰²æ€§èƒ½å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
        return save_path
    
    def visualize_latency_vs_bandwidth(self, bandwidths, results_by_bandwidth, save_path=None):
        """å¯è§†åŒ–ä¸åŒå¸¦å®½ä¸‹çš„æ¨ç†æ—¶å»¶
        
        Args:
            bandwidths: å¸¦å®½åˆ—è¡¨(MBps)
            results_by_bandwidth: æ¯ä¸ªå¸¦å®½å¯¹åº”çš„æœ€ä¼˜åˆ†å‰²ç»“æœ
            save_path: å›¾è¡¨ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸ºNoneè‡ªåŠ¨ç”Ÿæˆ
        """
        if not results_by_bandwidth:
            self.logger.warning("æ²¡æœ‰æä¾›æ€§èƒ½æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå¯è§†åŒ–")
            return None
        
        # æå–æ•°æ®
        latencies = [result['result']['total_time'] * 1000 for result in results_by_bandwidth]  # è½¬æ¢ä¸ºms
        strategies = []
        
        for result in results_by_bandwidth:
            if result['strategy'] == 'cloud_only':
                strategies.append('cloud')
            elif result['strategy'].startswith('edge_only'):
                strategies.append('edge')
            else:
                strategies.append('split')
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 6))
        
        # ç»˜åˆ¶ä¸»çº¿å›¾
        plt.plot(bandwidths, latencies, 'o-', color='blue', linewidth=2)
        plt.xlabel('Bandwidth (MBps)', fontsize=12)
        plt.ylabel('Inference Latency (ms)', fontsize=12)
        plt.title(f'Inference Latency vs. Bandwidth - {self.model_name}, BS={self.batch_size}', fontsize=14)
        plt.grid(True, linestyle='--', alpha=0.7)
        
        # æ·»åŠ ç­–ç•¥æ ‡æ³¨
        for i, (bw, latency, strategy) in enumerate(zip(bandwidths, latencies, strategies)):
            color = {'cloud': 'blue', 'edge': 'green', 'split': 'red'}[strategy]
            plt.scatter(bw, latency, color=color, s=100, zorder=5)
            
            # åªåœ¨éƒ¨åˆ†ç‚¹ä¸Šæ·»åŠ æ ‡ç­¾ï¼Œé¿å…æ‹¥æŒ¤
            if i % max(1, len(bandwidths)//10) == 0:
                plt.annotate(strategy, (bw, latency), 
                            textcoords="offset points", 
                            xytext=(0,10), 
                            ha='center',
                            fontsize=9)
        
        # æ·»åŠ å›¾ä¾‹
        from matplotlib.lines import Line2D
        legend_elements = [
            Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Cloud Only'),
            Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Edge Only'),
            Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Split Execution')
        ]
        plt.legend(handles=legend_elements, loc='upper right')
        
        # ä¿å­˜å›¾è¡¨
        if save_path is None:
            save_path = os.path.join('result', f'latency_vs_bandwidth_{self.model_name}_{self.batch_size}.png')
        
        plt.savefig(save_path, dpi=150)
        plt.close()
        
        self.logger.info(f"æ—¶å»¶ä¸å¸¦å®½å…³ç³»å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
        return save_path
    
    def visualize_latency_vs_batchsize(self, batch_sizes, results_by_batchsize, save_path=None):
        """å¯è§†åŒ–ä¸åŒæ‰¹å¤„ç†å¤§å°ä¸‹çš„æ¨ç†æ—¶å»¶
        
        Args:
            batch_sizes: æ‰¹å¤„ç†å¤§å°åˆ—è¡¨
            results_by_batchsize: æ¯ä¸ªæ‰¹å¤„ç†å¤§å°å¯¹åº”çš„æœ€ä¼˜åˆ†å‰²ç»“æœ
            save_path: å›¾è¡¨ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸ºNoneè‡ªåŠ¨ç”Ÿæˆ
        """
        if not results_by_batchsize:
            self.logger.warning("æ²¡æœ‰æä¾›æ€§èƒ½æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå¯è§†åŒ–")
            return None
        
        # æå–æ•°æ®
        latencies = [result['result']['total_time'] * 1000 for result in results_by_batchsize]  # è½¬æ¢ä¸ºms
        strategies = []
        
        for result in results_by_batchsize:
            if result['strategy'] == 'cloud_only':
                strategies.append('cloud')
            elif result['strategy'].startswith('edge_only'):
                strategies.append('edge')
            else:
                strategies.append('split')
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 6))
        
        # ç»˜åˆ¶ä¸»çº¿å›¾
        plt.plot(batch_sizes, latencies, 'o-', color='purple', linewidth=2)
        plt.xlabel('Batch Size', fontsize=12)
        plt.ylabel('Inference Latency (ms)', fontsize=12)
        plt.title(f'Inference Latency vs. Batch Size - {self.model_name}, BW={self.network_bandwidth/1e6:.1f}MBps', fontsize=14)
        plt.grid(True, linestyle='--', alpha=0.7)
        
        # æ·»åŠ ç­–ç•¥æ ‡æ³¨
        for i, (bs, latency, strategy) in enumerate(zip(batch_sizes, latencies, strategies)):
            color = {'cloud': 'blue', 'edge': 'green', 'split': 'red'}[strategy]
            plt.scatter(bs, latency, color=color, s=100, zorder=5)
            
            # åªåœ¨éƒ¨åˆ†ç‚¹ä¸Šæ·»åŠ æ ‡ç­¾ï¼Œé¿å…æ‹¥æŒ¤
            if i % max(1, len(batch_sizes)//10) == 0:
                plt.annotate(strategy, (bs, latency), 
                            textcoords="offset points", 
                            xytext=(0,10), 
                            ha='center',
                            fontsize=9)
        
        # æ·»åŠ å›¾ä¾‹
        from matplotlib.lines import Line2D
        legend_elements = [
            Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Cloud Only'),
            Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Edge Only'),
            Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Split Execution')
        ]
        plt.legend(handles=legend_elements, loc='upper left')
        
        # ä¿å­˜å›¾è¡¨
        if save_path is None:
            save_path = os.path.join('result', f'latency_vs_batchsize_{self.model_name}_{int(self.network_bandwidth/1e6)}mbps.png')
        
        plt.savefig(save_path, dpi=150)
        plt.close()
        
        self.logger.info(f"æ—¶å»¶ä¸æ‰¹å¤„ç†å¤§å°å…³ç³»å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
        return save_path
    def compare_methods(self, method1_func, method2_func, method1_name="Method 1", method2_name="Method 2"):
        """æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„æ€§èƒ½
        
        Args:
            method1_func: ç¬¬ä¸€ç§æ–¹æ³•çš„å‡½æ•°(æ— å‚æ•°ï¼Œè¿”å›ç»“æœå­—å…¸)
            method2_func: ç¬¬äºŒç§æ–¹æ³•çš„å‡½æ•°(æ— å‚æ•°ï¼Œè¿”å›ç»“æœå­—å…¸)
            method1_name: ç¬¬ä¸€ç§æ–¹æ³•çš„åç§°
            method2_name: ç¬¬äºŒç§æ–¹æ³•çš„åç§°
            
        Returns:
            åŒ…å«æ¯”è¾ƒç»“æœçš„å­—å…¸
        """
        self.logger.info(f"å¼€å§‹æ¯”è¾ƒ {method1_name} å’Œ {method2_name} çš„æ€§èƒ½...")
        
        # 1. æ‰§è¡Œç¬¬ä¸€ç§æ–¹æ³•
        start_time1 = time.time()
        result1 = method1_func()
        execution_time1 = time.time() - start_time1
        
        self.logger.info(f"{method1_name} ç”¨æ—¶: {execution_time1:.4f}ç§’")
        self.logger.info(f"{method1_name} ç­–ç•¥: {result1['strategy']}")
        self.logger.info(f"æ€»æ—¶å»¶: {result1['result']['total_time']*1000:.2f}ms")

        # 2. æ‰§è¡Œç¬¬äºŒç§æ–¹æ³•
        start_time2 = time.time()
        result2 = method2_func()
        execution_time2 = time.time() - start_time2
        
        self.logger.info(f"{method2_name} ç”¨æ—¶: {execution_time2:.4f}ç§’")
        self.logger.info(f"{method2_name} ç­–ç•¥: {result2['strategy']}")
        self.logger.info(f"æ€»æ—¶å»¶: {result2['result']['total_time']*1000:.2f}ms")
        
        # 3. æ¯”è¾ƒç»“æœ
        time_diff = (result2['result']['total_time'] - result1['result']['total_time']) / result1['result']['total_time'] * 100
        speed_up = execution_time1 / execution_time2 if execution_time2 > 0 else float('inf')
        
        self.logger.info("===== æ¯”è¾ƒç»“æœ =====")
        self.logger.info(f"æ—¶å»¶å·®å¼‚: {time_diff:.2f}%")
        self.logger.info(f"{method2_name} é€Ÿåº¦æå‡: {speed_up:.2f}å€")
        
        # å¯è§†åŒ–æ¯”è¾ƒç»“æœ
        self._plot_method_comparison(result1, result2, execution_time1, execution_time2, 
                                    method1_name, method2_name)
        
        # ä¿å­˜è¯¦ç»†æ¯”è¾ƒç»“æœ
        comparison_data = {
            'model_name': self.model_name,
            'batch_size': self.batch_size,
            'bandwidth_mbps': self.network_bandwidth/1e6,
            'method1': {
                'name': method1_name,
                'execution_time_s': execution_time1,
                'latency_ms': result1['result']['total_time'] * 1000,
                'strategy': result1['strategy'],
                'edge_node': result1.get('edge_node'),
                'bottleneck': result1.get('bottleneck')
            },
            'method2': {
                'name': method2_name,
                'execution_time_s': execution_time2,
                'latency_ms': result2['result']['total_time'] * 1000,
                'strategy': result2['strategy'],
                'edge_node': result2.get('edge_node'),
                'bottleneck': result2.get('bottleneck')
            },
            'comparison': {
                'latency_diff_percent': time_diff,
                'speed_up_factor': speed_up
            }
        }
        
        # ä¿å­˜æ¯”è¾ƒç»“æœ
        output_path = os.path.join('data', f'method_comparison_{self.model_name}_{self.batch_size}_{int(self.network_bandwidth/1e6)}.json')
        with open(output_path, 'w') as f:
            json.dump(comparison_data, f, indent=2)
        
        self.logger.info(f"æ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ° {output_path}")
        
        return comparison_data
    
    def _plot_method_comparison(self, result1, result2, time1, time2, method1_name="Method 1", method2_name="Method 2"):
        """ç»˜åˆ¶æ–¹æ³•æ¯”è¾ƒçš„å¯è§†åŒ–ç»“æœ
        
        Args:
            result1: ç¬¬ä¸€ç§æ–¹æ³•çš„ç»“æœ
            result2: ç¬¬äºŒç§æ–¹æ³•çš„ç»“æœ
            time1: ç¬¬ä¸€ç§æ–¹æ³•çš„æ‰§è¡Œæ—¶é—´
            time2: ç¬¬äºŒç§æ–¹æ³•çš„æ‰§è¡Œæ—¶é—´
            method1_name: ç¬¬ä¸€ç§æ–¹æ³•çš„åç§°
            method2_name: ç¬¬äºŒç§æ–¹æ³•çš„åç§°
        """
        # 1. å‡†å¤‡æ€§èƒ½æ•°æ®
        methods = [method1_name, method2_name]
        latency = [result1['result']['total_time']*1000, result2['result']['total_time']*1000]  # ms
        execution_time = [time1, time2]  # seconds
        
        # 2. åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # æ—¶å»¶å¯¹æ¯”
        axes[0].bar(methods, latency, color=['#3498db', '#e74c3c'])
        axes[0].set_title('Inference Latency (ms)')
        axes[0].set_ylabel('Milliseconds')
        axes[0].grid(axis='y', linestyle='--', alpha=0.7)
        
        # æ‰§è¡Œæ—¶é—´å¯¹æ¯” (å¯¹æ•°å°ºåº¦)
        axes[1].bar(methods, execution_time, color=['#3498db', '#e74c3c'])
        axes[1].set_title('Method Execution Time (s)')
        axes[1].set_ylabel('Seconds (log scale)')
        axes[1].set_yscale('log')
        axes[1].grid(axis='y', linestyle='--', alpha=0.7)
        
        # æ·»åŠ å…·ä½“æ•°å€¼æ ‡ç­¾
        for ax, data in zip(axes, [latency, execution_time]):
            for i, v in enumerate(data):
                ax.text(i, v, f"{v:.2f}", ha='center', va='bottom')
        
        # æ·»åŠ æ ‡é¢˜
        fig.suptitle(f'Method Comparison - {self.model_name}, BS={self.batch_size}, BW={self.network_bandwidth/1e6}MBps', 
                    fontsize=14)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.88)
        
        # ä¿å­˜å›¾è¡¨
        output_path = os.path.join('result', f'method_comparison_{self.model_name}_{self.batch_size}_{int(self.network_bandwidth/1e6)}.png')
        plt.savefig(output_path, dpi=150)
        plt.close()
        
        self.logger.info(f"æ–¹æ³•æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜: {output_path}")
    
    def run_cross_test(self, test_func, test_batch_sizes=None, test_bandwidths=None, label="CrossTest"):
        """åœ¨ä¸åŒæ‰¹å¤„ç†å¤§å°å’Œå¸¦å®½ä¸‹è¿è¡Œæµ‹è¯•
        
        Args:
            test_func: è¦è¿è¡Œçš„æµ‹è¯•å‡½æ•°ï¼Œå‚æ•°ä¸º(batch_size, bandwidth)
            test_batch_sizes: è¦æµ‹è¯•çš„æ‰¹å¤„ç†å¤§å°åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNoneè‡ªåŠ¨ç”Ÿæˆ
            test_bandwidths: è¦æµ‹è¯•çš„å¸¦å®½åˆ—è¡¨(MBps)ï¼Œé»˜è®¤ä¸ºNoneè‡ªåŠ¨ç”Ÿæˆ
            label: æµ‹è¯•æ ‡ç­¾ï¼Œç”¨äºä¿å­˜ç»“æœ
            
        Returns:
            åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        # å¦‚æœæœªæä¾›æµ‹è¯•å‚æ•°ï¼Œåˆ™ä½¿ç”¨ä»£è¡¨æ€§æ ·æœ¬
        if test_batch_sizes is None:
            test_batch_sizes = [1, 2, 4, 8, 16, 32, 64]  # ä½¿ç”¨ä»£è¡¨æ€§çš„æ‰¹å¤„ç†å¤§å°
        
        if test_bandwidths is None:
            test_bandwidths = [1, 2, 4, 8, 16, 32, 64, 128]  # ä½¿ç”¨ä»£è¡¨æ€§çš„å¸¦å®½(MBps)
        
        self.logger.info(f"å¼€å§‹äº¤å‰æµ‹è¯• {label}: {len(test_batch_sizes)}ä¸ªæ‰¹å¤„ç†å¤§å° Ã— {len(test_bandwidths)}ä¸ªå¸¦å®½")
        
        # å‡†å¤‡ç»“æœå®¹å™¨
        results = {
            'model_name': self.model_name,
            'batch_sizes': test_batch_sizes,
            'bandwidths': test_bandwidths,
            'test_label': label,
            'results_matrix': []  # äºŒç»´çŸ©é˜µ [batch_size][bandwidth]
        }
        
        # åˆå§‹åŒ–ç»“æœçŸ©é˜µ
        for _ in range(len(test_batch_sizes)):
            results['results_matrix'].append([None] * len(test_bandwidths))
        
        # ä¿å­˜åŸå§‹é…ç½®
        original_bw = self.network_bandwidth
        original_bs = self.batch_size
        
        # è®°å½•è¿›åº¦
        total_tests = len(test_batch_sizes) * len(test_bandwidths)
        completed_tests = 0
        start_time = time.time()
        
        # å¼€å§‹æµ‹è¯•
        try:
            for bs_idx, bs in enumerate(test_batch_sizes):
                for bw_idx, bw in enumerate(test_bandwidths):
                    # æ›´æ–°ç¯å¢ƒé…ç½®
                    self.network_bandwidth = bw * 10**6  # è½¬æ¢ä¸ºBps
                    self.batch_size = bs
                    self.inference_time_cache = {}  # æ¸…é™¤ç¼“å­˜
                    
                    self.logger.info(f"æµ‹è¯•é…ç½®: æ‰¹å¤„ç†å¤§å°={bs}, å¸¦å®½={bw}MBps")
                    
                    # è¿è¡Œæµ‹è¯•å‡½æ•°
                    try:
                        test_result = test_func(bs, bw * 10**6)
                        results['results_matrix'][bs_idx][bw_idx] = test_result
                    
                    except Exception as e:
                        self.logger.error(f"æµ‹è¯•é…ç½®(bs={bs}, bw={bw})å‡ºé”™: {str(e)}")
                        self.logger.exception(e)
                        results['results_matrix'][bs_idx][bw_idx] = {'error': str(e)}
                    
                    # æ›´æ–°è¿›åº¦
                    completed_tests += 1
                    progress = completed_tests / total_tests * 100
                    elapsed = time.time() - start_time
                    eta = (elapsed / completed_tests) * (total_tests - completed_tests) if completed_tests > 0 else 0
                    
                    self.logger.info(f"è¿›åº¦: {progress:.1f}% ({completed_tests}/{total_tests}), ETA: {eta/60:.1f}åˆ†é’Ÿ")
        
        finally:
            # æ¢å¤åŸå§‹é…ç½®
            self.network_bandwidth = original_bw
            self.batch_size = original_bs
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        output_path = os.path.join('data', f'{label.lower()}_{self.model_name}.json')
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
            
        self.logger.info(f"äº¤å‰æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° {output_path}")
        
        return results
    
    def save_to_json(self, data, filename, directory='data'):
        """å°†æ•°æ®ä¿å­˜ä¸ºJSONæ–‡ä»¶
        
        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            filename: æ–‡ä»¶å
            directory: ä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºdata
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        os.makedirs(directory, exist_ok=True)
        file_path = os.path.join(directory, filename)
        
        try:
            with open(file_path, 'w') as f:
                json.dump(data, f, indent=2)
            self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
            return file_path
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {str(e)}")
            return None
    
    def load_from_json(self, filename, directory='data'):
        """ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®
        
        Args:
            filename: æ–‡ä»¶å
            directory: åŠ è½½ç›®å½•ï¼Œé»˜è®¤ä¸ºdata
            
        Returns:
            åŠ è½½çš„æ•°æ®ï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è¿”å›None
        """
        file_path = os.path.join(directory, filename)
        
        if not os.path.exists(file_path):
            self.logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            self.logger.info(f"æ•°æ®å·²ä» {file_path} åŠ è½½")
            return data
        except Exception as e:
            self.logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
            return None