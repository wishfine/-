{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GPUS = [0,1]\n",
    "#环境设置\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(gpu_number) for gpu_number in SELECTED_GPUS])\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "\"\"\"\n",
    "https://github.com/tensorflow/tensorflow/issues/34415#issuecomment-895336269\n",
    "https://stackoverflow.com/questions/59616436/how-to-reset-initialization-in-tensorflow-2\n",
    "\"\"\"\n",
    "MAX_CPU_THREADS = 16\n",
    "tf.config.threading.set_intra_op_parallelism_threads(MAX_CPU_THREADS)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(MAX_CPU_THREADS)\n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "GPUS = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in GPUS:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "DISTRIBUTED_STRATEGY = tf.distribute.MirroredStrategy(\n",
    "    cross_device_ops=tf.distribute.NcclAllReduce(),\n",
    "    devices=['/gpu:%d' % index for index in range(len(SELECTED_GPUS))]\n",
    ")\n",
    "\n",
    "NUM_GPUS = DISTRIBUTED_STRATEGY.num_replicas_in_sync\n",
    "\n",
    "print('Number of devices: {}'.format(NUM_GPUS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "import psutil  # 用于监控系统内存\n",
    "import gc      # 垃圾回收\n",
    "\n",
    "# 导入基类\n",
    "from dqn_split_basic import BaseDNNSplitter\n",
    "from download_resnet50 import patch_dnn_splitter\n",
    "\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "# 创建数据和结果目录\n",
    "def ensure_directories():\n",
    "    \"\"\"确保数据和结果目录存在\"\"\"\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    os.makedirs('result', exist_ok=True)\n",
    "    os.makedirs('log', exist_ok=True)\n",
    "\n",
    "# 初始化目录\n",
    "ensure_directories()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSplitTrainer(BaseDNNSplitter):\n",
    "    \"\"\"用于训练DQN的DNN分割系统\n",
    "    \n",
    "    继承BaseDNNSplitter基类，专注于DQN训练功能\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"初始化DQN训练器\n",
    "        \n",
    "        Args:\n",
    "            config: 配置字典，包含模型、环境和算法参数\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        self.logger = config.get('logger', logging.getLogger(self.__class__.__name__))\n",
    "        \n",
    "        # DQN参数\n",
    "        self.memory = deque(maxlen=2000)  # 经验回放缓冲区\n",
    "        \n",
    "        # 核心DQN超参数\n",
    "        self.gamma = 0.95    # 折扣因子\n",
    "        self.epsilon = 1.0   # 探索率\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.logger.info(f\"DQN参数设置完成: γ={self.gamma}, ε初始值={self.epsilon}, ε最小值={self.epsilon_min}\")\n",
    "        \n",
    "        # 先创建主模型\n",
    "        self.logger.info(\"初始化主DQN模型...\")\n",
    "        self.model = self.build_dqn_model()\n",
    "        self.logger.info(\"主DQN模型已初始化\")\n",
    "\n",
    "        # 然后创建目标网络\n",
    "        self.logger.info(\"初始化DQN目标网络...\")\n",
    "        self.target_model = self.build_dqn_model()\n",
    "\n",
    "        # 现在可以安全地调用update_target_model()了\n",
    "        self.update_target_model()\n",
    "        \n",
    "        self.logger.info(f\"DQN训练器初始化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(self, state, action, reward, next_state, done):\n",
    "    \"\"\"将经验存储到记忆库\"\"\"\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "    if len(self.memory) % 100 == 0:\n",
    "        self.logger.debug(f\"记忆库大小: {len(self.memory)}\")\n",
    "\n",
    "def act(self, state):\n",
    "    \"\"\"根据当前状态选择动作\"\"\"\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "        # 探索：随机选择动作\n",
    "        return random.randrange(len(self.natural_bottlenecks) + 2)\n",
    "    \n",
    "    # 利用：根据Q值选择动作\n",
    "    act_values = self.model.predict(state)\n",
    "    return np.argmax(act_values[0])\n",
    "\n",
    "def update_target_model(self):\n",
    "    \"\"\"更新目标模型权重\"\"\"\n",
    "    self.target_model.set_weights(self.model.get_weights())\n",
    "    self.logger.info(\"目标网络权重已更新\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(self, batch_size):\n",
    "    \"\"\"从记忆库中随机抽取批量样本进行训练 (改进的DDQN实现)\n",
    "    \n",
    "    Args:\n",
    "        batch_size: 批大小\n",
    "        \n",
    "    Returns:\n",
    "        训练损失\n",
    "    \"\"\"\n",
    "    if len(self.memory) < batch_size:\n",
    "        self.logger.debug(f\"记忆库样本不足，当前: {len(self.memory)}，需要: {batch_size}\")\n",
    "        return 0\n",
    "        \n",
    "    minibatch = random.sample(self.memory, batch_size)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            # DDQN: 主网络选择动作，目标网络评估该动作\n",
    "            a = np.argmax(self.model.predict(next_state,verbose=0)[0])  # 使用主网络选择动作\n",
    "            target = reward + self.gamma * self.target_model.predict(next_state,verbose=0)[0][a]  # 使用目标网络评估\n",
    "        \n",
    "        target_f = self.model.predict(state, verbose=0)\n",
    "        original_q = target_f[0][action]\n",
    "        \n",
    "        # 计算TD误差\n",
    "        td_error = abs(target - original_q)\n",
    "        self.logger.info(f\"TD误差: {td_error:.4f}, 原Q值: {original_q:.4f}, 目标Q值: {target:.4f}\")\n",
    "        \n",
    "        # 设置目标Q值\n",
    "        target_f[0][action] = target\n",
    "        \n",
    "        # 根据TD误差决定训练强度\n",
    "        if td_error > 0.5:  # 大误差，加强学习\n",
    "            history = self.model.fit(state, target_f, epochs=2, verbose=0)\n",
    "        else:\n",
    "            history = self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        losses.append(history.history['loss'][0])\n",
    "    \n",
    "    # 衰减探索率 (在主训练函数中已有更复杂的探索策略，这里可以删除)\n",
    "    # if self.epsilon > self.epsilon_min:\n",
    "    #     self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    return np.mean(losses) if losses else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_or_calculate_best_actions(self, max_bandwidth, fixed_bs):\n",
    "    \"\"\"加载或计算不同带宽下的最佳动作\n",
    "    \n",
    "    首先尝试从缓存文件加载，如果文件不存在则重新计算并保存\n",
    "    \n",
    "    Args:\n",
    "        max_bandwidth: 最大带宽值(MBps)\n",
    "        fixed_bs: 固定的批处理大小\n",
    "        \n",
    "    Returns:\n",
    "        best_actions_dict: 不同带宽下的最佳动作字典\n",
    "    \"\"\"\n",
    "    # 构建缓存文件路径\n",
    "    cache_filename = f\"best_actions_{self.model_name}_bs{fixed_bs}.json\"\n",
    "    cache_path = os.path.join('data', cache_filename)\n",
    "    \n",
    "    # 检查缓存文件是否存在\n",
    "    if os.path.exists(cache_path):\n",
    "        self.logger.info(f\"发现缓存的最佳动作文件: {cache_filename}，正在加载...\")\n",
    "        try:\n",
    "            with open(cache_path, 'r') as f:\n",
    "                best_actions = json.load(f)\n",
    "                \n",
    "            # 将字符串键转换为整数键\n",
    "            best_actions = {int(k): v for k, v in best_actions.items()}\n",
    "            self.logger.info(f\"成功加载缓存的最佳动作，共 {len(best_actions)} 个带宽点\")\n",
    "            return best_actions\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"加载缓存文件失败: {str(e)}，将重新计算最佳动作\")\n",
    "    else:\n",
    "        self.logger.info(f\"未找到缓存文件 {cache_filename}，将计算所有带宽点的最佳动作...\")\n",
    "    \n",
    "    # 计算所有带宽点的最佳动作\n",
    "    best_actions = {}\n",
    "    \n",
    "    self.logger.info(f\"开始计算所有带宽点的最优动作...\")\n",
    "    # 预先计算所有带宽点\n",
    "    for bw in range(1, max_bandwidth + 1):\n",
    "        self.network_bandwidth = bw * 10**6  # 转换为Bps\n",
    "        self.inference_time_cache = {}  # 清除缓存\n",
    "        \n",
    "        self.logger.info(f\"计算带宽 {bw}MBps 的最优动作...\")\n",
    "        best_action, best_reward, all_rewards_dict = self._find_best_action_for_bandwidth(bw)\n",
    "        best_actions[bw] = {\n",
    "            'action': best_action,\n",
    "            'reward': best_reward,\n",
    "            'all_rewards': all_rewards_dict\n",
    "        }\n",
    "        self.logger.info(f\"带宽 {bw}MBps 的最佳动作: {self.describe_action(best_action)}, 奖励: {best_reward:.4f}\")\n",
    "    \n",
    "    # 保存计算结果\n",
    "    self.logger.info(f\"所有带宽点的最优动作计算完成，正在保存到 {cache_path}\")\n",
    "    try:\n",
    "        # 确保data目录存在\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        \n",
    "        # 保存为JSON文件\n",
    "        with open(cache_path, 'w') as f:\n",
    "            json.dump(best_actions, f, indent=2)\n",
    "        self.logger.info(f\"最佳动作数据已保存到 {cache_path}\")\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"保存最佳动作数据失败: {str(e)}\")\n",
    "    \n",
    "    return best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_adaptive(self, fixed_bs=1, max_bandwidth=100, max_episodes=1000, target_accuracy=0.90):\n",
    "    \"\"\"改进的DDQN训练过程，增强模型在不同带宽下选择最优动作的能力\n",
    "    \n",
    "    Args:\n",
    "        fixed_bs: 固定的批处理大小，默认为1\n",
    "        max_bandwidth: 最大带宽值(MBps)，默认为100\n",
    "        max_episodes: 最大训练轮数，默认为1000\n",
    "        target_accuracy: 早停机制的目标准确率，当达到此准确率时停止训练，默认为0.95\n",
    "    \"\"\"\n",
    "    self.logger.info(f\"开始改进版DDQN训练: 固定批处理大小={fixed_bs}, 训练轮数={max_episodes}, 目标准确率={target_accuracy:.2%}\")\n",
    "\n",
    "    \n",
    "    # 记录训练过程\n",
    "    training_results = {}\n",
    "    all_rewards = []\n",
    "    all_losses = []\n",
    "    all_accuracies = []  # 新增：记录准确率历史\n",
    "    \n",
    "    # 保存原始配置\n",
    "    original_bw = self.network_bandwidth\n",
    "    original_bs = self.batch_size\n",
    "    original_epsilon = self.epsilon\n",
    "    \n",
    "    # 固定批处理大小\n",
    "    self.batch_size = fixed_bs\n",
    "    self.logger.info(f\"训练使用固定批大小: {fixed_bs}\")\n",
    "    \n",
    "    # 定义带宽采样策略：更均衡地采样各带宽区间\n",
    "    bandwidth_ranges = [\n",
    "        (1, 10),    # 低带宽区间\n",
    "        (10, 20),   # 中低带宽区间\n",
    "        (20, 60),   # 中带宽区间\n",
    "        (60, 100)   # 高带宽区间\n",
    "    ]\n",
    "    range_weights = [0.25, 0.25, 0.25, 0.25]  # 均衡采样\n",
    "    \n",
    "    # 优先经验回放缓冲区增强版\n",
    "    priority_memory = {}  # 带宽范围 -> 经验列表\n",
    "    for bw_range in bandwidth_ranges:\n",
    "        priority_memory[bw_range] = []\n",
    "    \n",
    "    # 性能跟踪\n",
    "    best_overall_reward = float('-inf')\n",
    "    no_improvement_count = 0\n",
    "    best_model_path = os.path.join('models', f'dqn_best_{self.model_name}_bs{fixed_bs}.h5')\n",
    "    \n",
    "    # 新增：早停参数\n",
    "    last_accuracy = 0.0\n",
    "    early_stop = False\n",
    "    \n",
    "    # 【方案1】添加最小训练轮数\n",
    "    min_episodes = 100# 至少训练30%的轮数\n",
    "    self.logger.info(f\"设置最小训练轮数: {min_episodes} (即使准确率达标也会继续训练到此轮数)\")\n",
    "    \n",
    "    # 【方案2】加载或计算所有带宽的最优动作\n",
    "    self.logger.info(f\"准备最佳动作数据...\")\n",
    "    self.best_actions_per_bandwidth = self._load_or_calculate_best_actions(max_bandwidth, fixed_bs)\n",
    "    self.logger.info(f\"最佳动作数据准备完成，共 {len(self.best_actions_per_bandwidth)} 个带宽点\")\n",
    "    \n",
    "    try:\n",
    "        for episode in range(max_episodes):\n",
    "            # 自适应带宽采样策略\n",
    "            if episode < max_episodes * 0.2:  # 前20%均匀探索\n",
    "                selected_range_idx = np.random.choice(len(bandwidth_ranges), p=range_weights)\n",
    "                min_bw, max_bw = bandwidth_ranges[selected_range_idx]\n",
    "                bw = np.random.randint(min_bw, max_bw + 1)\n",
    "            else:  # 后80%聚焦错误率高的带宽区间\n",
    "                # 每100轮进行一次全带宽扫描找出模型表现差的区间\n",
    "                if episode % 100 == 0 and episode > 0:\n",
    "                    error_weights = self._analyze_model_errors()\n",
    "                    if any(error_weights):  # 如果有错误权重数据\n",
    "                        selected_range_idx = np.random.choice(len(bandwidth_ranges), p=error_weights)\n",
    "                        min_bw, max_bw = bandwidth_ranges[selected_range_idx]\n",
    "                        bw = np.random.randint(min_bw, max_bw + 1)\n",
    "                    else:  # 默认均匀采样\n",
    "                        selected_range_idx = np.random.choice(len(bandwidth_ranges), p=range_weights)\n",
    "                        min_bw, max_bw = bandwidth_ranges[selected_range_idx]\n",
    "                        bw = np.random.randint(min_bw, max_bw + 1)\n",
    "                else:  # 正常采样\n",
    "                    selected_range_idx = np.random.choice(len(bandwidth_ranges), p=range_weights)\n",
    "                    min_bw, max_bw = bandwidth_ranges[selected_range_idx]\n",
    "                    bw = np.random.randint(min_bw, max_bw + 1)\n",
    "            \n",
    "            self.network_bandwidth = bw * 10**6  # 转换为Bps\n",
    "            self.inference_time_cache = {}  # 清除缓存\n",
    "            \n",
    "            # 使用预计算的最佳动作\n",
    "            best_action = self.best_actions_per_bandwidth[bw]['action']\n",
    "            best_reward = self.best_actions_per_bandwidth[bw]['reward']\n",
    "            \n",
    "            # 获取当前状态并执行动作\n",
    "            state = self.get_state()\n",
    "            action = self.act(state)  # 通过DQN策略选择动作\n",
    "            \n",
    "            action_result = self.execute_action(action)\n",
    "            reward = self.calculate_reward(action_result['result'])\n",
    "            next_state = self.get_state()\n",
    "            \n",
    "            # 将经验存入记忆库\n",
    "            self.remember(state, action, reward, next_state, False)\n",
    "            \n",
    "            # 优先存储到带宽区间特定的记忆库\n",
    "            for (min_r, max_r) in bandwidth_ranges:\n",
    "                if min_r <= bw <= max_r:\n",
    "                    # 每个区间记忆库限制大小\n",
    "                    if len(priority_memory[(min_r, max_r)]) >= 500:\n",
    "                        priority_memory[(min_r, max_r)].pop(0)\n",
    "                    priority_memory[(min_r, max_r)].append((state, action, reward, next_state, False))\n",
    "                    break\n",
    "            \n",
    "            # 训练模型\n",
    "            current_loss = 0\n",
    "            if len(self.memory) > 32:\n",
    "                current_loss = self.replay(32)\n",
    "                all_losses.append(current_loss)\n",
    "            \n",
    "            all_rewards.append(reward)\n",
    "             # 在循环末尾添加打印当前epsilon值的代码\n",
    "            self.logger.info(f\"Episode {episode+1}/{max_episodes}, 当前探索率(epsilon): {self.epsilon:.4f}\")\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            \n",
    "            # 打印每轮的信息\n",
    "            is_optimal = \"是\" if action == best_action else \"否\"\n",
    "            gap = 0 if action == best_action else (best_reward - reward)\n",
    "            self.logger.info(f\"Episode {episode+1} 结果: 动作={action}({self.describe_action(action)}), \"\n",
    "                          f\"奖励={reward:.4f}, 是最优动作: {is_optimal}, 差距: {gap:.4f}, \"\n",
    "                          f\"损失={current_loss:.6f}\")\n",
    "            \n",
    "            # 周期性更新目标网络\n",
    "            if episode % 5 == 0:\n",
    "                self.update_target_model()\n",
    "                self.logger.info(f\"Episode {episode+1}: 目标网络已更新\")\n",
    "            \n",
    "            # 保存性能最好的模型\n",
    "            if reward > best_overall_reward and episode > max_episodes * 0.1:\n",
    "                best_overall_reward = reward\n",
    "                no_improvement_count = 0\n",
    "                self.model.save_weights(best_model_path)\n",
    "                self.logger.info(f\"发现更好的模型，奖励: {reward:.4f}\")\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "            \n",
    "            # 定期检查模型准确性（选择正确动作的比例）\n",
    "            if episode % 20 == 0 and episode > 0:\n",
    "                accuracy = self._evaluate_model_accuracy()\n",
    "                all_accuracies.append(accuracy)  # 记录准确率\n",
    "                self.logger.info(f\"Episode {episode+1}: 模型准确率: {accuracy:.2f}%\")\n",
    "                \n",
    "                # 【方案1】添加最小训练轮数条件\n",
    "                if accuracy >= target_accuracy * 100 and episode >= min_episodes:\n",
    "                    self.logger.info(f\"达到目标准确率 {accuracy:.2f}% >= {target_accuracy*100:.2f}%，且已满足最小训练轮数，提前结束训练!\")\n",
    "                    early_stop = True\n",
    "                    break\n",
    "                \n",
    "                last_accuracy = accuracy\n",
    "            # 定期执行垃圾回收\n",
    "            if episode % 100 == 0:\n",
    "                gc.collect()\n",
    "        \n",
    "        # 训练结束，恢复最佳模型\n",
    "        if os.path.exists(best_model_path):\n",
    "            self.model.load_weights(best_model_path)\n",
    "            self.logger.info(f\"训练结束，已加载性能最佳模型\")\n",
    "        \n",
    "        # 保存训练结果\n",
    "        self._save_training_results(all_rewards, all_losses, max_episodes, fixed_bs)\n",
    "        self._plot_training_progress(all_rewards, all_losses, fixed_bs)\n",
    "        \n",
    "    finally:\n",
    "        # 恢复原始配置\n",
    "        self.network_bandwidth = original_bw\n",
    "        self.batch_size = original_bs\n",
    "        self.epsilon = original_epsilon\n",
    "    \n",
    "    return training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_best_action_for_bandwidth(self, bw):\n",
    "    \"\"\"针对特定带宽尝试所有可能的动作，找出最佳动作\n",
    "    \n",
    "    Args:\n",
    "        bw: 带宽值(MBps)\n",
    "        \n",
    "    Returns:\n",
    "        best_action: 最佳动作索引\n",
    "        best_reward: 最佳奖励值\n",
    "        all_rewards: 所有动作的奖励值字典\n",
    "    \"\"\"\n",
    "    self.network_bandwidth = bw * 10**6  # 转换为Bps\n",
    "    self.inference_time_cache = {}  # 清除缓存\n",
    "    best_action = None\n",
    "    best_reward = float('-inf')\n",
    "    all_rewards = {}\n",
    "    \n",
    "    # 尝试所有可能的动作\n",
    "    num_actions = len(self.natural_bottlenecks) + 2\n",
    "    for action in range(num_actions):\n",
    "        action_result = self.execute_action(action)\n",
    "        reward = self.calculate_reward(action_result['result'])\n",
    "        all_rewards[action] = {\n",
    "            'reward': float(reward),\n",
    "            'description': self.describe_action(action)\n",
    "        }\n",
    "        \n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "            best_action = action\n",
    "    \n",
    "    return best_action, best_reward, all_rewards\n",
    "\n",
    "def _priority_replay(self, minibatch):\n",
    "    \"\"\"带优先级的经验回放\n",
    "    \n",
    "    Args:\n",
    "        minibatch: 经验样本批次\n",
    "        \n",
    "    Returns:\n",
    "        训练损失\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            # DDQN: 主网络选择动作，目标网络评估该动作\n",
    "            a = np.argmax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target = reward + self.gamma * self.target_model.predict(next_state, verbose=0)[0][a]\n",
    "        \n",
    "        # 获取当前Q值估计\n",
    "        target_f = self.model.predict(state, verbose=0)\n",
    "        original_q = target_f[0][action]\n",
    "        target_f[0][action] = target\n",
    "        \n",
    "        # 计算TD误差\n",
    "        td_error = abs(target - original_q)\n",
    "        \n",
    "        # 根据TD误差调整学习率\n",
    "        if td_error > 0.5:  # 大误差，更强调学习\n",
    "            history = self.model.fit(state, target_f, epochs=2, verbose=0)\n",
    "        else:\n",
    "            history = self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        losses.append(history.history['loss'][0])\n",
    "    \n",
    "    return np.mean(losses) if losses else 0\n",
    "\n",
    "def _analyze_model_errors(self):\n",
    "    \"\"\"分析模型在不同带宽区间的错误，返回应当重点关注的区间权重\n",
    "    \n",
    "    Returns:\n",
    "        区间权重列表\n",
    "    \"\"\"\n",
    "    bandwidth_ranges = [\n",
    "        (1, 10),    # 低带宽区间\n",
    "        (10, 20),   # 中低带宽区间\n",
    "        (20, 60),   # 中带宽区间\n",
    "        (60, 100)   # 高带宽区间\n",
    "    ]\n",
    "    \n",
    "    # 默认平均权重\n",
    "    if not hasattr(self, 'bandwidth_errors'):\n",
    "        return [0.25, 0.25, 0.25, 0.25]\n",
    "    \n",
    "    # 计算每个区间的错误率\n",
    "    error_rates = []\n",
    "    for min_bw, max_bw in bandwidth_ranges:\n",
    "        errors = 0\n",
    "        total = 0\n",
    "        for bw in range(min_bw, max_bw + 1):\n",
    "            if bw in self.bandwidth_errors:\n",
    "                errors += self.bandwidth_errors[bw]['errors']\n",
    "                total += self.bandwidth_errors[bw]['total']\n",
    "        \n",
    "        if total > 0:\n",
    "            error_rates.append(errors / total)\n",
    "        else:\n",
    "            error_rates.append(0.25)  # 默认值\n",
    "    \n",
    "    # 归一化错误率作为权重\n",
    "    if sum(error_rates) > 0:\n",
    "        weights = [rate / sum(error_rates) for rate in error_rates]\n",
    "        return weights\n",
    "    else:\n",
    "        return [0.25, 0.25, 0.25, 0.25]  # 默认平均权重\n",
    "\n",
    "def _evaluate_model_accuracy(self):\n",
    "    \"\"\"评估模型在各带宽下选择最佳动作的准确率\n",
    "    \n",
    "    Returns:\n",
    "        准确率百分比\n",
    "    \"\"\"\n",
    "    if not hasattr(self, 'best_actions_per_bandwidth') or not self.best_actions_per_bandwidth:\n",
    "        return 0\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # 初始化或重置带宽错误记录\n",
    "    if not hasattr(self, 'bandwidth_errors'):\n",
    "        self.bandwidth_errors = {}\n",
    "    \n",
    "    # 评估带宽采样点\n",
    "    test_bandwidths = list(range(1, 101))\n",
    "    temp_epsilon = self.epsilon\n",
    "    self.epsilon = 0  # 禁用探索\n",
    "    \n",
    "    for bw in test_bandwidths:\n",
    "        if bw in self.best_actions_per_bandwidth:\n",
    "            best_action = self.best_actions_per_bandwidth[bw]['action']\n",
    "            \n",
    "            # 使用模型预测动作\n",
    "            self.network_bandwidth = bw * 10**6\n",
    "            self.inference_time_cache = {}  # 清除缓存\n",
    "            state = self.get_state()\n",
    "            predicted_action = self.act(state)\n",
    "            \n",
    "            # 记录准确性\n",
    "            if predicted_action == best_action:\n",
    "                correct += 1\n",
    "            \n",
    "            # 记录错误统计\n",
    "            if bw not in self.bandwidth_errors:\n",
    "                self.bandwidth_errors[bw] = {'errors': 0, 'total': 0}\n",
    "            \n",
    "            if predicted_action != best_action:\n",
    "                self.bandwidth_errors[bw]['errors'] += 1\n",
    "            self.bandwidth_errors[bw]['total'] += 1\n",
    "            \n",
    "            total += 1\n",
    "    \n",
    "    # 恢复探索率\n",
    "    self.epsilon = temp_epsilon\n",
    "    \n",
    "    return (correct / total * 100) if total > 0 else 0\n",
    "\n",
    "def _focused_training_for_problem_bandwidths(self):\n",
    "    \"\"\"针对问题带宽区间进行集中训练\"\"\"\n",
    "    if not hasattr(self, 'bandwidth_errors') or not self.bandwidth_errors:\n",
    "        return\n",
    "    \n",
    "    problem_bandwidths = []\n",
    "    for bw, stats in self.bandwidth_errors.items():\n",
    "        if stats['total'] > 0 and stats['errors'] / stats['total'] > 0.5:\n",
    "            problem_bandwidths.append(bw)\n",
    "    \n",
    "    if not problem_bandwidths:\n",
    "        return\n",
    "    \n",
    "    self.logger.info(f\"对问题带宽进行集中训练: {problem_bandwidths}\")\n",
    "    \n",
    "    # 保存当前状态\n",
    "    original_epsilon = self.epsilon\n",
    "    \n",
    "    # 针对问题带宽进行集中训练\n",
    "    for bw in problem_bandwidths:\n",
    "        if bw not in self.best_actions_per_bandwidth:\n",
    "            continue\n",
    "        \n",
    "        best_action = self.best_actions_per_bandwidth[bw]['action']\n",
    "        self.network_bandwidth = bw * 10**6\n",
    "        self.inference_time_cache = {}  # 清除缓存\n",
    "        \n",
    "        # 集中训练10次\n",
    "        for _ in range(10):\n",
    "            state = self.get_state()\n",
    "            next_state = state  # 简化\n",
    "            reward = self.best_actions_per_bandwidth[bw]['reward']\n",
    "            \n",
    "            # 直接使用目标值进行监督训练\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][best_action] = reward\n",
    "            \n",
    "            # 其他动作设为较低值\n",
    "            for a in range(len(target_f[0])):\n",
    "                if a != best_action:\n",
    "                    target_f[0][a] = reward - 0.5\n",
    "            \n",
    "            # 加强训练\n",
    "            self.model.fit(state, target_f, epochs=5, verbose=0)\n",
    "    \n",
    "    # 恢复探索率\n",
    "    self.epsilon = original_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 修改 _save_training_results 函数\n",
    "def _save_training_results(self, rewards, losses, episodes, fixed_bs=1):\n",
    "    \"\"\"保存训练结果到JSON文件\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    result = {\n",
    "        'model_name': self.model_name,\n",
    "        'batch_size': fixed_bs,\n",
    "        'episodes': episodes,\n",
    "        'completed_episodes': len(rewards),\n",
    "        'final_epsilon': float(self.epsilon),\n",
    "        'rewards': [float(r) for r in rewards],\n",
    "        'losses': [float(l) if not np.isnan(l) else None for l in losses]\n",
    "    }\n",
    "    \n",
    "    # 修改：文件名包含模型名称和批处理大小\n",
    "    result_path = os.path.join('data', f'dqn_training_{self.model_name}_bs{fixed_bs}.json')\n",
    "    with open(result_path, 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    \n",
    "    # 修改：文件名包含模型名称和批处理大小\n",
    "    final_model_path = os.path.join('models', f'dqn_{self.model_name}_bs{fixed_bs}_final.h5')\n",
    "    self.model.save_weights(final_model_path)\n",
    "    \n",
    "    self.logger.info(f\"训练结果已保存到 {result_path}\")\n",
    "    self.logger.info(f\"最终模型已保存到 {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_training_progress(self, rewards, losses, fixed_bs=1):\n",
    "    \"\"\"绘制训练进度图（不进行平滑处理）\"\"\"\n",
    "    self.logger.info(\"绘制训练过程曲线图\")\n",
    "    \n",
    "    # 创建图表\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # 绘制奖励曲线\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.arange(len(rewards)), rewards, 'b-', linewidth=1.5, label='Reward')\n",
    "    plt.title('DDQN Training Reward', fontsize=14)\n",
    "    plt.xlabel('Episodes', fontsize=12)\n",
    "    plt.ylabel('Average Reward', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    if losses:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(np.arange(len(losses)), losses, 'r-', linewidth=1.5, label='Loss')\n",
    "        plt.title('Training Loss', fontsize=14)\n",
    "        plt.xlabel('Episode', fontsize=12)\n",
    "        plt.ylabel('Loss', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join('result', f\"dqn_training_{self.model_name}_bs{fixed_bs}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    self.logger.info(f\"训练过程图已保存到 {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_optimal_split_dqn(self):\n",
    "    \"\"\"使用DQN模型预测最优的分割策略\n",
    "    \n",
    "    Returns:\n",
    "        包含最优分割策略信息的字典\n",
    "    \"\"\"\n",
    "    # 获取当前环境状态\n",
    "    state = self.get_state()\n",
    "    \n",
    "    # 使用DQN模型预测最佳动作\n",
    "    action, q_values = self.predict_action_with_qvalues(state, self.model)\n",
    "    \n",
    "     # 解释动作但不执行它\n",
    "    action_info = self.interpret_action(action)\n",
    "    \n",
    "    # 记录日志\n",
    "    self.logger.info(f\"DQN预测分割策略: {action_info['strategy']}\")\n",
    "    if action_info['strategy'] == 'split':\n",
    "        bottleneck_name = action_info.get('bottleneck', 'unknown')\n",
    "        self.logger.info(f\"分割点: {bottleneck_name}\")\n",
    "    \n",
    "    # 添加Q值信息\n",
    "    action_info['q_values'] = q_values.tolist() if hasattr(q_values, 'tolist') else q_values\n",
    "    action_info['action_index'] = action\n",
    "    \n",
    "    return action_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action_with_qvalues(self, state, model):\n",
    "    \"\"\"使用给定模型预测动作和对应的Q值\n",
    "    \n",
    "    Args:\n",
    "        state: 当前状态\n",
    "        model: DQN模型\n",
    "        \n",
    "    Returns:\n",
    "        action: 预测的动作\n",
    "        q_values: 所有动作的Q值\n",
    "    \"\"\"\n",
    "    q_values = model.predict(state, verbose=0)[0]\n",
    "    action = np.argmax(q_values)\n",
    "    return action, q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_trained_model(self, fixed_bs=1, max_bandwidth=100):\n",
    "    \"\"\"验证已训练模型在各带宽下的性能\n",
    "    \n",
    "    Args:\n",
    "        fixed_bs: 固定的批处理大小\n",
    "        max_bandwidth: 最大带宽值(MBps)\n",
    "    \n",
    "    Returns:\n",
    "        验证结果字典\n",
    "    \"\"\"\n",
    "    self.logger.info(f\"Validating model performance for batch size={fixed_bs}\")\n",
    "    \n",
    "    # 保存原始配置\n",
    "    original_bw = self.network_bandwidth\n",
    "    original_bs = self.batch_size\n",
    "    original_epsilon = self.epsilon\n",
    "    \n",
    "    # 禁用探索\n",
    "    self.epsilon = 0.0\n",
    "    \n",
    "    # 固定批处理大小\n",
    "    self.batch_size = fixed_bs\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        for bw in range(1, max_bandwidth + 1):\n",
    "            self.network_bandwidth = bw * 10**6  # 转换为Bps\n",
    "            self.logger.info(f\"带宽={bw}MBps\")\n",
    "            self.inference_time_cache = {}  # 清除缓存\n",
    "            # 获取状态\n",
    "            state = self.get_state()\n",
    "            \n",
    "            # 使用模型预测最佳动作\n",
    "            action = self.act(state)  # epsilon为0，总是选择最高Q值的动作\n",
    "            \n",
    "            # 执行动作并计算奖励\n",
    "            action_result = self.execute_action(action)\n",
    "            reward = self.calculate_reward(action_result['result'])\n",
    "            latency = action_result['result'].get('total_time', 0)\n",
    "            \n",
    "            # 记录结果\n",
    "            results[bw] = {\n",
    "                'action': int(action),\n",
    "                'action_description': self.describe_action(action),\n",
    "                'reward': float(reward),\n",
    "                'latency': float(latency)\n",
    "            }\n",
    "            \n",
    "            if bw % 10 == 0 or bw == 1 or bw == max_bandwidth:\n",
    "                self.logger.info(f\"Validation bandwidth={bw}MBps: Action={self.describe_action(action)}, \"\n",
    "                               f\"Reward={reward:.4f}, Latency={latency:.4f}s\")\n",
    "    \n",
    "    finally:\n",
    "        # 恢复原始配置\n",
    "        self.network_bandwidth = original_bw\n",
    "        self.batch_size = original_bs\n",
    "        self.epsilon = original_epsilon\n",
    "    \n",
    "    # 保存验证结果，加入模型名称\n",
    "    validation_filename = f\"validation_results_{self.model_name}_bs{fixed_bs}.json\"\n",
    "    with open(os.path.join('result', validation_filename), 'w') as f:\n",
    "        # 转换为可JSON序列化格式\n",
    "        json_results = {str(k): v for k, v in results.items()}\n",
    "        json.dump(json_results, f, indent=4)\n",
    "    \n",
    "    self.logger.info(f\"Validation results saved as {validation_filename}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将已定义的方法绑定到类上\n",
    "DQNSplitTrainer.remember = remember\n",
    "DQNSplitTrainer.act = act\n",
    "DQNSplitTrainer.update_target_model = update_target_model\n",
    "DQNSplitTrainer.replay = replay\n",
    "DQNSplitTrainer.train_dqn_adaptive = train_dqn_adaptive\n",
    "DQNSplitTrainer._find_best_action_for_bandwidth = _find_best_action_for_bandwidth\n",
    "DQNSplitTrainer._priority_replay = _priority_replay\n",
    "DQNSplitTrainer._analyze_model_errors = _analyze_model_errors\n",
    "DQNSplitTrainer._evaluate_model_accuracy = _evaluate_model_accuracy\n",
    "DQNSplitTrainer._focused_training_for_problem_bandwidths = _focused_training_for_problem_bandwidths\n",
    "# 添加缺失的方法绑定\n",
    "DQNSplitTrainer._save_training_results = _save_training_results\n",
    "DQNSplitTrainer._plot_training_progress = _plot_training_progress\n",
    "DQNSplitTrainer.predict_optimal_split_dqn = predict_optimal_split_dqn\n",
    "DQNSplitTrainer.predict_action_with_qvalues = predict_action_with_qvalues\n",
    "DQNSplitTrainer.validate_trained_model = validate_trained_model\n",
    "# 添加新的方法绑定\n",
    "DQNSplitTrainer._load_or_calculate_best_actions = _load_or_calculate_best_actions\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数，执行DQN训练过程\"\"\"\n",
    "    #model_name = 'vit-l32'\n",
    "    #model_name = 'vgg-16'\n",
    "    model_name = 'ResNet50'\n",
    "    #model_name = 'efficientnet-b4'\n",
    "    fixed_bs = 5  # 固定批处理大小为1\n",
    "    if model_name == 'ResNet50':\n",
    "        patch_dnn_splitter()\n",
    "    # 配置日志\n",
    "    log_filename = os.path.join('log', f\"dqn_training_{model_name}_bs{fixed_bs}.log\")\n",
    "    # 创建处理器\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    console_handler = logging.StreamHandler()\n",
    "    # 设置格式\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    #logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "    \n",
    "    # 配置根日志记录器\n",
    "    global logger  # 使用全局变量，以便其他方法可以访问\n",
    "    logger = logging.getLogger('DQNSplitTrainer')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # 清除可能存在的旧处理器\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"========== 开始运行DQN模型训练系统 ==========\")\n",
    "\n",
    "        # 配置\n",
    "        config = {\n",
    "            'model_name': model_name,\n",
    "            'image_size': 224,\n",
    "            'edge_nodes': [\n",
    "                {\n",
    "                    'name': 'edge1',\n",
    "                    'device': '/GPU:1',  \n",
    "                },\n",
    "            ],\n",
    "            'cloud_device': '/GPU:0',\n",
    "            'batch_size': list(range(1, 65)),  # 批处理大小范围\n",
    "            'max_bandwidth': 128 * 10**6,  # 最大带宽 128 MBps\n",
    "            'min_bandwidth': 1 * 10**6,    # 最小带宽 1 MBps\n",
    "            'bandwidth_step': 1 * 10**6,    # 带宽步长 1 MBps\n",
    "        }\n",
    "        \n",
    "        # 创建DQN训练器实例\n",
    "        trainer = DQNSplitTrainer(config)\n",
    "        \n",
    "        # 1. 训练DQN模型\n",
    "        logger.info(\"\\n=== 开始训练DQN模型 ===\")\n",
    "        training_results = trainer.train_dqn_adaptive(fixed_bs=fixed_bs, max_episodes=800)\n",
    "\n",
    "        # 验证模型\n",
    "        validation_results = trainer.validate_trained_model(fixed_bs=fixed_bs)\n",
    "        \n",
    "        logger.info(\"\\n===== 所有训练过程完成 =====\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"执行过程中发生错误: {str(e)}\", exc_info=True)\n",
    "        raise e\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n",
    "    print(\"DQN训练完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
